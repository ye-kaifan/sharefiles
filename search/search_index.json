{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kai-Fan's blogs","text":"<ul> <li>\u901a\u8fc7\u4e3b\u9898\u548c\u76ee\u5f55\u6d4f\u89c8\u6587\u7ae0\u3002</li> <li>\u7535\u8111\u7aef\uff1a\u5728\u4e0a\u65b9\u6807\u7b7e\u680f\u9009\u62e9\u4e3b\u9898\uff0c\u5728\u5de6\u4fa7\u76ee\u5f55\u9009\u62e9\u6587\u7ae0\u3002</li> <li> <p>\u79fb\u52a8\u7aef\uff1a\u70b9\u51fb\u5de6\u4e0a\u89d2\u56fe\u6807\u9009\u62e9\u4e3b\u9898\u548c\u6587\u7ae0\u3002</p> </li> <li> <p>\u641c\u7d22\u5173\u952e\u8bcd\u67e5\u627e\u6587\u7ae0\u3002</p> </li> </ul>"},{"location":"LLM4phys/finetune/","title":"\u5fae\u8c03\u7269\u7406\u4e13\u5bb6\u5927\u6a21\u578b","text":""},{"location":"LLM4phys/finetune/#_2","title":"\u76f8\u5173\u7684\u8bba\u6587","text":"<ul> <li>Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models</li> <li>Abstract: Transformer \u662f\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7531\u5806\u53e0\u7684\u6ce8\u610f\u529b\u548c\u70b9\u72b6\u3001\u5168\u8fde\u63a5\u5c42\u7ec4\u6210\uff0c\u65e8\u5728\u5904\u7406\u5e8f\u5217\u6570\u636e\u3002Transformer \u4e0d\u4ec5\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u800c\u4e14\u6700\u8fd1\u4e5f\u6fc0\u53d1\u4e86\u4e00\u6ce2\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5e94\u7528\u7814\u7a76\u7684\u65b0\u6d6a\u6f6e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u89c6\u89c9 Transformer\uff08ViT\uff09\u88ab\u5fae\u8c03\u4ee5\u9884\u6d4b\u4e8c\u7ef4\u4f0a\u8f9b\u6a21\u578b\u6a21\u62df\u7684\u72b6\u6001\u53d8\u91cf\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u6765\u81ea\u4f0a\u8f9b\u6a21\u578b\u5bf9\u5e94\u4e8e\u5404\u79cd\u8fb9\u754c\u6761\u4ef6\u548c\u6e29\u5ea6\u7684\u5c11\u91cf\u5fae\u6001\u56fe\u50cf\u65f6\uff0cViT \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002\u8fd9\u9879\u5de5\u4f5c\u63a2\u8ba8\u4e86 ViT \u5728\u5176\u4ed6\u6a21\u62df\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5e76\u5f15\u5165\u4e86\u5173\u4e8e\u6ce8\u610f\u529b\u56fe\u5982\u4f55\u5b66\u4e60\u4e0d\u540c\u73b0\u8c61\u7684\u6f5c\u5728\u7269\u7406\u7684\u7814\u7a76\u65b9\u5411\u3002</li> <li> <p>https://arxiv.org/abs/2109.13925</p> </li> <li> <p>Can a CNN trained on the Ising model detect the phase transition of the $q$-state Potts model?</p> </li> <li>Abstract: \u91c7\u7528\u5728\u4e8c\u7ef4\u4f0a\u8f9b\u6a21\u578b\u7684\u81ea\u65cb\u914d\u7f6e\u548c\u6e29\u5ea6\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u6df1\u5ea6 CNN\uff09\uff0c\u6211\u4eec\u7814\u7a76\u6df1\u5ea6 CNN \u662f\u5426\u53ef\u4ee5\u68c0\u6d4b\u4e8c\u7ef4 $q$-\u6001 Potts \u6a21\u578b\u7684\u76f8\u53d8\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u81ea\u65cb\u53d8\u91cf ${0, 1, \\ldots, \\lfloor q/2 \\rfloor - 1}$ \u548c ${\\lfloor q/2 \\rfloor, \\ldots, q-1}$ \u5206\u522b\u66ff\u6362\u4e3a ${0}$ \u548c ${1}$\uff0c\u751f\u6210 $q$-\u6001 Potts \u6a21\u578b ($q \\geq 3$) \u7684\u81ea\u65cb\u914d\u7f6e\u7684\u4e8c\u503c\u56fe\u50cf\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u56fe\u50cf\u8f93\u5165\u5230\u8bad\u7ec3\u597d\u7684 CNN \u4e2d\uff0c\u8f93\u51fa\u9884\u6d4b\u7684\u6e29\u5ea6\u3002$q$-\u6001 Potts \u6a21\u578b\u7684\u4e8c\u503c\u56fe\u50cf\u4e0e\u4f0a\u8f9b\u81ea\u65cb\u914d\u7f6e\u5b8c\u5168\u4e0d\u540c\uff0c\u5c24\u5176\u662f\u5728\u76f8\u53d8\u6e29\u5ea6\u65f6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684 CNN \u6a21\u578b\u5e76\u672a\u8bad\u7ec3\u6709\u5173\u76f8\u662f\u5426\u6709\u6709\u5e8f/\u65e0\u5e8f\u7684\u4fe1\u606f\uff0c\u800c\u662f\u901a\u8fc7\u4f0a\u8f9b\u81ea\u65cb\u914d\u7f6e\u53ca\u5176\u751f\u6210\u7684\u6e29\u5ea6\u6807\u7b7e\u8fdb\u884c\u5929\u771f\u7684\u8bad\u7ec3\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6df1\u5ea6 CNN \u53ef\u4ee5\u4ee5\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u5230\u76f8\u53d8\u70b9\uff0c\u65e0\u8bba\u76f8\u53d8\u7684\u7c7b\u578b\u5982\u4f55\u3002\u6211\u4eec\u8fd8\u5728\u9ad8\u6e29\u533a\u57df\u53d1\u73b0\uff0cCNN \u6839\u636e\u5185\u80fd\u8f93\u51fa\u6e29\u5ea6\uff0c\u800c\u5728\u4f4e\u6e29\u533a\u57df\uff0c\u8f93\u51fa\u53d6\u51b3\u4e8e\u78c1\u5316\u5f3a\u5ea6\u4ee5\u53ca\u53ef\u80fd\u8fd8\u6709\u5185\u80fd\u3002\u7136\u800c\uff0c\u5728\u76f8\u53d8\u70b9\u9644\u8fd1\uff0cCNN \u53ef\u80fd\u4f7f\u7528\u66f4\u4e00\u822c\u7684\u56e0\u7d20\u6765\u68c0\u6d4b\u76f8\u53d8\u70b9\u3002</li> <li> <p>https://arxiv.org/abs/2104.03632v3</p> </li> <li> <p>Quantum many-body physics calculations with large language models</p> </li> <li>Abstract: \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\uff0c\u5305\u62ec\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u4e2d\uff0c\u5c55\u793a\u4e86\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0cLLMs\u53ef\u4ee5\u51c6\u786e\u6267\u884c\u7406\u8bba\u7269\u7406\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u5173\u952e\u8ba1\u7b97\u3002\u6211\u4eec\u5173\u6ce8\u91cf\u5b50\u7269\u7406\u5b66\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff1a\u54c8\u7279\u91cc-\u798f\u514b\u65b9\u6cd5\uff0c\u5b83\u9700\u8981\u901a\u8fc7\u591a\u6b65\u89e3\u6790\u8ba1\u7b97\u63a8\u5bfc\u51fa\u8fd1\u4f3c\u54c8\u5bc6\u987f\u91cf\u548c\u76f8\u5e94\u7684\u81ea\u6d3d\u65b9\u7a0b\u3002\u4e3a\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u8ba1\u7b97\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u591a\u6b65\u63d0\u793a\u6a21\u677f\uff0c\u5c06\u89e3\u6790\u8ba1\u7b97\u5206\u89e3\u4e3a\u6807\u51c6\u5316\u7684\u6b65\u9aa4\uff0c\u5e76\u4e3a\u7279\u5b9a\u95ee\u9898\u4fe1\u606f\u7559\u51fa\u5360\u4f4d\u7b26\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 GPT-4 \u5728\u6267\u884c\u8fc7\u53bb\u5341\u5e74 15 \u7bc7\u8bba\u6587\u7684\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u901a\u8fc7\u7ea0\u6b63\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5b83\u5728 13 \u4e2a\u6848\u4f8b\u4e2d\u53ef\u4ee5\u6b63\u786e\u63a8\u5bfc\u51fa\u6700\u7ec8\u7684\u54c8\u7279\u91cc-\u798f\u514b\u54c8\u5bc6\u987f\u91cf\u3002\u6c47\u603b\u6240\u6709\u7814\u7a76\u8bba\u6587\uff0c\u6211\u4eec\u53d1\u73b0\u5355\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u7684\u6267\u884c\u5e73\u5747\u5f97\u5206\u4e3a 87.5\uff08\u6ee1\u5206 100 \u5206\uff09\u3002 \u6211\u4eec\u8fdb\u4e00\u6b65\u4f7f\u7528LLMs\u6765\u7f13\u89e3\u6b64\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\uff08i\uff09\u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u4fe1\u606f\u4ee5\u586b\u5145\u6a21\u677f\uff1b\uff08ii\uff09\u81ea\u52a8\u8bc4\u5206\u8ba1\u7b97\u6b65\u9aa4\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002</li> <li> <p>https://arxiv.org/abs/2403.03154</p> </li> <li> <p>PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators</p> </li> <li>Abstract: \u7c92\u5b50\u52a0\u901f\u5668\u9886\u57df\u7684\u5bfc\u822a\u968f\u7740\u8fd1\u671f\u8d21\u732e\u7684\u6fc0\u589e\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u4e9b\u590d\u6742\u7684\u8bbe\u5907\u5373\u4f7f\u5728\u5355\u4e2a\u8bbe\u65bd\u5185\u4e5f\u96be\u4ee5\u7406\u89e3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 PACuna\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u516c\u5f00\u53ef\u7528\u7684\u52a0\u901f\u5668\u8d44\u6e90\uff08\u5982\u4f1a\u8bae\u3001\u9884\u5370\u672c\u548c\u4e66\u7c4d\uff09\u8fdb\u884c\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u3002\u6211\u4eec\u81ea\u52a8\u5316\u4e86\u6570\u636e\u6536\u96c6\u548c\u95ee\u9898\u751f\u6210\uff0c\u4ee5\u6700\u5c0f\u5316\u4e13\u5bb6\u7684\u53c2\u4e0e\uff0c\u5e76\u4f7f\u4ee3\u7801\u53ef\u4f9b\u4f7f\u7528\u3002PACuna \u5728\u89e3\u51b3\u7531\u4e13\u5bb6\u9a8c\u8bc1\u7684\u52a0\u901f\u5668\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8868\u660e\uff0c\u901a\u8fc7\u5fae\u8c03\u6280\u672f\u6587\u672c\u548c\u81ea\u52a8\u751f\u6210\u7684\u8bed\u6599\u5e93\u6765\u9002\u5e94\u79d1\u5b66\u9886\u57df\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4ea7\u751f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u56de\u7b54\u4e00\u4e9b\u5546\u4e1a\u53ef\u7528\u52a9\u624b\u65e0\u6cd5\u56de\u7b54\u7684\u7279\u5b9a\u95ee\u9898\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5355\u4e2a\u8bbe\u65bd\u7684\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u3002</li> <li>https://arxiv.org/abs/2310.19106</li> </ul>"},{"location":"LLM4phys/finetune/#unslothlora","title":"\u7528unsloth\u8fdb\u884cLoRA\u9ad8\u6548\u5fae\u8c03","text":"<ol> <li>\u51c6\u5907json\u6216jsonl\u683c\u5f0f\u7684\u95ee\u7b54\u5bf9\u6570\u636e\u96c6\u3002</li> <li>https://huggingface.co/datasets/camel-ai/physics</li> <li>\u8bbe\u7f6e\u8bad\u7ec3\u8d85\u53c2\u6570\uff0clearning rate, batch size and num epochs.</li> </ol>"},{"location":"LLM4phys/trans4lqcd/","title":"Transformer for LQCD","text":"<p>\u6709\u4e24\u7bc7\u6587\u7ae0\u90fd\u662fAkio\u7684\uff0c\u628a\u89c4\u8303\u534f\u53d8Transformer\u67b6\u6784\u7528\u4e8eself-learning Monte Carlo\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\uff0c\u56e0\u4e3aTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u63d0\u53d6\u5230\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7684\u7279\u5f81\u3002\u8fd8\u6709\u4e00\u7bc7\u6587\u7ae0\u63d0\u5230\uff0c\u76f8\u6bd4DNN\uff0cmodified Transformer \u5728\u4f30\u7b97\u4f2a\u6807\u91cf\u80f6\u7403\u7684\u8d28\u91cf\u65f6\uff0c\u8bef\u5dee\u66f4\u5c0f\u3002</p>"},{"location":"LLM4phys/trans4lqcd/#cask-a-gauge-covariant-transformer-for-lattice-gauge-theory","title":"CASK: A Gauge Covariant Transformer for Lattice Gauge Theory","text":"<ul> <li>https://arxiv.org/abs/2501.16955</li> </ul> <p>We propose a Transformer neural network architecture specifically designed for lattice QCD, focusing on preserving the fundamental symmetries required in lattice gauge theory. The proposed architecture is gauge covariant/equivariant, ensuring it respects gauge symmetry on the lattice, and is also equivariant under spacetime symmetries such as rotations and translations on the lattice. A key feature of our approach lies in the attention matrix, which forms the core of the Transformer architecture. To preserve symmetries, we define the attention matrix using a Frobenius inner product between link variables and extended staples. This construction ensures that the attention matrix remains invariant under gauge transformations, thereby making the entire Transformer architecture covariant. We evaluated the performance of the gauge covariant Transformer in the context of self-learning HMC. Numerical experiments show that the proposed architecture achieves higher performance compared to the gauge covariant neural networks, demonstrating its potential to improve lattice QCD calculations.</p> <ul> <li>\u628a\u89c4\u8303\u534f\u53d8Transformer \u7528\u5728self-learning HMC\u91cc\u9762\uff0c\u63d0\u9ad8\u4e86\u63a5\u53d7\u7387\u3002\u6027\u80fd\u4f18\u4e8e\u4ed6\u4eec\u4e4b\u524d\u5f00\u53d1\u7684gauge covariant neural networks (\u201cadaptive stout\u201d)\u3002</li> <li>the gauge covariant Transformer architecture CASK \u4e0d\u4ec5\u80fd\u4fdd\u6301\u89c4\u8303\u4e0d\u53d8\u6027\uff0c\u8fd8\u80fd\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u5230\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7684\u7279\u5f81\u3002</li> </ul> <p>In our numerical experiments, the surrogate links generated by CASK successfully absorbed the differences arising from the modified massive Dirac operator, resulting in an improved acceptance rate. The method consistently outperformed the gauge covariant neural networks (\u201cadaptive stout\u201d) developed in our previous study, illustrating how the attention-based design can enhance expressivity. These findings suggest that the gauge covariant Transformer approach is a promising route toward more efficient and flexible simulations in lattice QCD. Future work will explore larger lattice volumes, extended loop structures in the attention matrix, and further optimization of the training process to fully leverage the potential of CASK.</p>"},{"location":"LLM4phys/trans4lqcd/#self-learning-monte-carlo-with-equivariant-transformer","title":"Self-learning Monte Carlo with equivariant Transformer","text":"<ul> <li>https://arxiv.org/abs/2306.11527</li> <li>Equivariant Transformer is all you need (proceedings) https://arxiv.org/abs/2310.13222</li> </ul> <p>Machine learning and deep learning have revolutionized computational physics, particularly the simulation of complex systems. Considering equivariance and long-range correlations is essential for simulating physical systems. Equivariance imposes a strong inductive bias on the probability distribution described by a machine learning model, while long-range correlation is important for understanding classical/quantum phase transitions. Inspired by Transformers used in large language models, which can treat long-range dependencies in the networks, we introduce a symmetry equivariant Transformer for self-learning Monte Carlo. We evaluate our architecture on a spin-fermion model (i.e., the double exchange model) on a two-dimensional lattice. Our results show that the proposed method overcomes the poor acceptance rates of linear models and exhibits a similar scaling law to large language models, with model quality monotonically increasing with the number of layers. Our work paves the way for the development of more accurate and efficient Monte Carlo algorithms with machine learning for simulating complex physical systems.</p> <ul> <li>the effective model with the Transformer architecture can capture the long-range correlation in the original model.</li> <li>Leveraging the extensive capacity of the attention layer, we successfully construct an effective model of the system using the equivariant Transformer. We find that the model with the attention blocks overcomes the poor acceptance rates of linear models and exhibits a similar scaling law as in large language models.</li> </ul>"},{"location":"LLM4phys/trans4lqcd/#estimation-of-the-pseudoscalar-glueball-mass-based-on-a-modified-transformer","title":"Estimation of the pseudoscalar glueball mass based on a modified Transformer","text":"<ul> <li>https://arxiv.org/abs/2408.13280</li> </ul> <p>A modified Transformer model is introduced for estimating the mass of pseudoscalar glueball in lattice QCD. The model takes as input a sequence of floating-point numbers with lengths ranging from 30 to 35 and produces a two-dimensional vector output. It integrates floating-point embeddings and positional encoding, and is trained using binary cross-entropy loss. The paper provides a detailed description of the model\u2019s components and training methods, and compares the performance of the traditional least squares method, the previously used deep neural network, and the modified Transformer in mass estimation. The results show that the modified Transformer model achieves greater accuracy in mass estimation than the traditional least squares method. Additionally, compared to the deep neural network, this model utilizes positional encoding and can handle input sequences of varying lengths, offering enhanced adaptability.</p> <ul> <li>\u76f8\u6bd4DNN\uff0cmodified Transformer \u5728\u4f30\u7b97\u4f2a\u6807\u91cf\u80f6\u7403\u7684\u8d28\u91cf\u65f6\uff0c\u8bef\u5dee\u66f4\u5c0f\u3002</li> </ul>"},{"location":"LLM4phys/trans4phys/","title":"Transformer for physics","text":"<ul> <li>\u4ec0\u4e48\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff1f</li> <li>Transformer \u7684\u539f\u7406</li> <li>\u4f7f\u7528 Transformer \u6765\u5b66\u4e60\u7269\u7406\u7684\u8bed\u8a00 \uff08\u6bd4\u5982\u9a6c\u5c14\u53ef\u592b\u94fe\uff09</li> <li>Rapid detection of phase transitions from Monte Carlo samples before equilibrium</li> <li>CASK: A Gauge Covariant Transformer for Lattice Gauge Theory</li> <li>Self-learning Monte Carlo with equivariant Transformer</li> <li>Equivariant Transformer is all you need</li> <li>Transformers for modeling physical systems<ul> <li>\u5b9e\u73b0\u65b9\u6cd5</li> </ul> </li> <li>\u4f7f\u7528 Transformer \u6765\u5b66\u4e60\u5206\u5b50\u548c\u86cb\u767d\u8d28\u7684\u8bed\u8a00\u7684\u6848\u4f8b<ul> <li>Scientific Large Language Models: A Survey on Biological \\&amp; Chemical Domains</li> <li>\u6838\u5fc3\u89c2\u70b9</li> </ul> </li> </ul>"},{"location":"LLM4phys/trans4phys/#_1","title":"\u4ec0\u4e48\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff1f","text":"<ul> <li>https://www.nvidia.cn/glossary/large-language-models/</li> </ul> <p>\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\u6765\u8bc6\u522b\u3001\u603b\u7ed3\u3001\u7ffb\u8bd1\u3001\u9884\u6d4b\u548c\u751f\u6210\u5185\u5bb9\u3002</p> <p>\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ee3\u8868\u4e86\u4e00\u7c7b\u540d\u4e3aTransformer\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002Transformer\u6a21\u578b\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8ffd\u8e2a\u5e8f\u5217\u6570\u636e (\u5982\u672c\u53e5\u4e2d\u7684\u5355\u8bcd) \u4e2d\u7684\u5173\u7cfb\u5b66\u4e60\u4e0a\u4e0b\u6587\u53ca\u5176\u542b\u4e49\u3002</p> <p>{width=80%} \u56fe\u7247\u6765\u6e90</p> <p>Transformer\u7531\u591a\u4e2aTransformer\u5757 (\u4e5f\u79f0\u4e3a\u5c42) \u7ec4\u6210\u3002\u4f8b\u5982\uff0cTransformer\u5177\u6709\u81ea\u6ce8\u610f\u5c42\u3001\u524d\u9988\u5c42\u548c\u5f52\u4e00\u5316\u5c42\uff0c\u6240\u6709\u8fd9\u4e9b\u5c42\u534f\u540c\u53d1\u6325\u4f5c\u7528\u6765\u89e3\u5bc6\u8f93\u5165\u5185\u5bb9\uff0c\u4ee5\u4fbf\u5728\u63a8\u7406\u65f6\u9884\u6d4b\u8f93\u51fa\u6d41\u3002\u8fd9\u4e9b\u5c42\u53ef\u4ee5\u8fdb\u884c\u5806\u53e0\uff0c\u5f62\u6210\u66f4\u6df1\u5c42\u7684Transformer\u548c\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\u3002Transformer\u6700\u65e9\u7531 Google \u5728 2017 \u5e74\u7684\u8bba\u6587\u201cAttention Is All You Need\u201d\u4e2d\u63d0\u51fa\u3002</p> <p>{width=80%}</p> <p>\u56fe 1: \u8f6c\u6362\u5668\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u3002</p> <p>\u6709\u4e24\u9879\u5173\u952e\u521b\u65b0\u4f7f\u5f97Transformer\u7279\u522b\u9002\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff1a\u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08Self-Attention\uff09\u3002</p> <p>\u4f4d\u7f6e\u7f16\u7801\u4f1a\u5d4c\u5165\u8f93\u5165\u5185\u5bb9\u5728\u7ed9\u5b9a\u5e8f\u5217\u4e2d\u7684\u987a\u5e8f\u3002\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u6709\u4e86\u4f4d\u7f6e\u7f16\u7801\uff0c\u5c31\u53ef\u4ee5\u4e0d\u6309\u987a\u5e8f\u5c06\u53e5\u5b50\u4e2d\u7684\u5355\u8bcd\u8f93\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4f1a\u5728\u5904\u7406\u8f93\u5165\u6570\u636e\u65f6\u4e3a\u8f93\u5165\u6570\u636e\u7684\u6bcf\u4e2a\u90e8\u5206\u5206\u914d\u4e00\u4e2a\u6743\u91cd\u3002\u6b64\u6743\u91cd\u8868\u793a\u76f8\u5e94\u8f93\u5165\u5185\u5bb9\u5728\u4e0a\u4e0b\u6587\u4e2d\u76f8\u5bf9\u4e8e\u5176\u4f59\u8f93\u5165\u5185\u5bb9\u7684\u91cd\u8981\u6027\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6a21\u578b\u4e0d\u518d\u9700\u8981\u5411\u6240\u6709\u8f93\u5165\u5185\u5bb9\u6295\u5165\u540c\u6837\u7684\u6ce8\u610f\u529b\uff0c\u800c\u662f\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u8f93\u5165\u5185\u5bb9\u4e2d\u771f\u6b63\u91cd\u8981\u7684\u90e8\u5206\u3002\u8fd9\u79cd\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5173\u6ce8\u8f93\u5165\u5185\u5bb9\u7684\u54ea\u4e9b\u90e8\u5206\u7684\u5177\u4f53\u8868\u793a\uff0c\u662f\u5728\u6a21\u578b\u7b5b\u9009\u548c\u5206\u6790\u5927\u91cf\u6570\u636e\u7684\u8fc7\u7a0b\u4e2d\u9010\u6e10\u5b66\u4e60\u5230\u7684\u3002</p> <p>\u5c06\u8fd9\u4e24\u79cd\u6280\u672f\u52a0\u4ee5\u7ed3\u5408\uff0c\u53ef\u4ee5\u5206\u6790\u4e0d\u540c\u5143\u7d20\u5982\u4f55\u5728\u957f\u8ddd\u79bb\u3001\u975e\u987a\u5e8f\u7684\u60c5\u51b5\u4e0b\u76f8\u4e92\u5f71\u54cd\u548c\u5173\u8054\u7684\u5fae\u5999\u65b9\u5f0f\u4e0e\u4e0a\u4e0b\u6587\u3002</p> <p>\u4ee5\u975e\u987a\u5e8f\u65b9\u5f0f\u5904\u7406\u6570\u636e\u7684\u80fd\u529b\u80fd\u591f\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u8f83\u5c0f\u7684\u540c\u6b65\u8ba1\u7b97\u3002\u81ea\u7136\uff0cGPU \u975e\u5e38\u9002\u5408\u5e76\u884c\u89e3\u51b3\u8fd9\u4e9b\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u4ece\u800c\u53ef\u4ee5\u5927\u89c4\u6a21\u5904\u7406\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u548c\u5e9e\u5927\u7684Transformer\u7f51\u7edc\u3002</p>"},{"location":"LLM4phys/trans4phys/#transformer","title":"Transformer \u7684\u539f\u7406","text":"<p>Transformer tutorials:</p> <ul> <li>https://jalammar.github.io/illustrated-transformer/</li> <li>\u4e2d\u6587\u7ffb\u8bd1\uff1ahttps://blog.csdn.net/qq_36667170/article/details/124359818</li> <li>https://github.com/bentrevett/pytorch-seq2seq/</li> <li>https://github.com/hyunwoongko/transformer</li> <li>https://github.com/tensorflow/tensor2tensor</li> </ul>"},{"location":"LLM4phys/trans4phys/#transformer_1","title":"\u4f7f\u7528 Transformer \u6765\u5b66\u4e60\u7269\u7406\u7684\u8bed\u8a00 \uff08\u6bd4\u5982\u9a6c\u5c14\u53ef\u592b\u94fe\uff09","text":"<p>Transformer \u80fd\u591f\u5b66\u4e60\u4efb\u4f55\u5e8f\u5217\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u9886\u57df\uff0c\u5e8f\u5217\u5c31\u662f\u4e00\u6bb5\u6587\u5b57\uff0c\u5728\u7269\u7406\u9886\u57df\uff0c\u5e8f\u5217\u53ef\u4ee5\u662f\u4e00\u6761\u9a6c\u5c14\u53ef\u592b\u94fe\u3002</p>"},{"location":"LLM4phys/trans4phys/#rapid-detection-of-phase-transitions-from-monte-carlo-samples-before-equilibrium","title":"Rapid detection of phase transitions from Monte Carlo samples before equilibrium","text":"<ul> <li>https://arxiv.org/abs/2206.13137</li> </ul> <p>\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5728\u76f8\u53d8\u7814\u7a76\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u9700\u957f\u65f6\u95f4\u5e73\u8861\u91c7\u6837\u7684\u74f6\u9888\u95ee\u9898\u3002\u9488\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u4e01\u6d01\u4f1f\u3001\u5510\u6d69\u91d1\u548c\u4f59\u6c38\u57fa\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u521b\u65b0\u8303\u5f0f\u3002\u8be5\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u91c7\u7528\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u7cfb\u7edf\u672a\u8fbe\u5230\u70ed\u529b\u5b66\u5e73\u8861\u7684\u8499\u7279\u5361\u6d1b\u7ec4\u6001\u6837\u672c\u4e2d\u63d0\u53d6\u6709\u6548\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u5bf9\u76f8\u53d8\u4e34\u754c\u6e29\u5ea6\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u548cTransformer\u67b6\u6784\u80fd\u591f\u901a\u8fc7\u89e3\u6790\u7ecf\u5178\u81ea\u65cb\u7cfb\u7edf\u7684\u6784\u578b\u5206\u5e03\u4ee5\u53ca\u91cf\u5b50\u7cfb\u7edf\u7684\u683c\u6797\u51fd\u6570\u7b49\u591a\u5143\u6570\u636e\u7279\u5f81\uff0c\u5728\u7cfb\u7edf\u672a\u8fbe\u5230\u70ed\u529b\u5b66\u5e73\u8861\u7684\u8499\u7279\u5361\u6d1b\u7ec4\u6001\u6837\u672c\u4e2d\u5b66\u4e60\u5e76\u5b8c\u6210\u4e0d\u540c\u7269\u76f8\u7684\u5206\u7c7b\u8bc6\u522b\uff0c\u5e76\u4ee5\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u76f8\u53d8\u4e34\u754c\u70b9\u3002</p>"},{"location":"LLM4phys/trans4phys/#cask-a-gauge-covariant-transformer-for-lattice-gauge-theory","title":"CASK: A Gauge Covariant Transformer for Lattice Gauge Theory","text":"<ul> <li>https://arxiv.org/abs/2501.16955</li> </ul> <p>\u8be5\u8bba\u6587\u300aCASK\uff1a\u7528\u4e8e\u6676\u683c\u89c4\u8303\u7406\u8bba\u7684\u89c4\u8303\u534f\u53d8 Transformer\u300b\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u52a0\u901f\u8ba1\u7b97\u5bc6\u96c6\u578b\u6676\u683c\u91cf\u5b50\u8272\u52a8\u529b\u5b66\uff08QCD\uff09\u6a21\u62df\u7684\u7a81\u7834\u6027\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u672c\u7269\u7406\u539f\u7406\u3002\u7531\u4e1c\u4eac\u5927\u5b66\u548c\u7406\u5316\u5b66\u7814\u7a76\u6240\u7b49\u65e5\u672c\u8457\u540d\u673a\u6784\u7684\u7814\u7a76\u5458 Yuki Nagai\u3001Hiroshi Ohno \u548c Akio Tomiya \u5f00\u53d1\uff0cCASK\uff08\u534f\u53d8\u6ce8\u610f\u529b\u4e0e Stout \u6838\uff09\u4ee3\u8868\u4e86\u5c06 Transformer \u67b6\u6784\u5e94\u7528\u4e8e\u7269\u7406\u95ee\u9898\u7684\u91cd\u8981\u8fdb\u5c55\u3002</p> <p>\u6676\u683c QCD \u8ba1\u7b97\uff0c\u5bf9\u4e8e\u7406\u89e3\u5f3a\u6838\u529b\u548c\u57fa\u672c\u7c92\u5b50\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u8fd9\u9879\u7814\u7a76\u89e3\u51b3\u4e86\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5982\u4f55\u5728\u4fdd\u6301\u89c4\u8303\u5bf9\u79f0\u6027\u7684\u540c\u65f6\u5229\u7528\u673a\u5668\u5b66\u4e60\u6765\u52a0\u901f\u8fd9\u4e9b\u8ba1\u7b97\uff0c\u89c4\u8303\u5bf9\u79f0\u6027\u662f\u4e00\u4e2a\u5fc5\u987b\u4fdd\u7559\u4ee5\u83b7\u5f97\u6709\u610f\u4e49\u7ed3\u679c\u7684\u57fa\u672c\u7269\u7406\u539f\u7406\u3002CASK \u67b6\u6784\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u672c\u8d28\u4e0a\u5177\u6709\u89c4\u8303\u534f\u53d8\u6027\u7684\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5de7\u5999\u5730\u89e3\u51b3\u4e86\u8fd9\u4e00\u77db\u76fe\u3002</p>"},{"location":"LLM4phys/trans4phys/#self-learning-monte-carlo-with-equivariant-transformer","title":"Self-learning Monte Carlo with equivariant Transformer","text":"<ul> <li>https://arxiv.org/abs/2306.11527</li> </ul> <p>\u672c\u6587\u7531\u65e5\u672c\u539f\u5b50\u80fd\u673a\u6784\u7684 Yuki Nagai \u548c\u56fd\u9645\u4e13\u4e1a\u6280\u672f\u5927\u5b66\u7684 Akio Tomiya \u63d0\u51fa\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c06 SLMC \u6846\u67b6\u4e0e\u7b49\u53d8 Transformer \u67b6\u6784\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u7269\u7406\u7cfb\u7edf\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u5173\u952e\u521b\u65b0\u662f\u5f15\u5165\u4e86\u4e00\u4e2a\u5c0a\u91cd\u6240\u6a21\u62df\u7269\u7406\u7cfb\u7edf\u57fa\u672c\u5bf9\u79f0\u6027\u7684\u7b49\u53d8 Transformer \u6a21\u578b\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u4f5c\u8005\u5c55\u793a\u4e86\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u51dd\u805a\u6001\u7269\u7406\u4e2d\u7684\u53cc\u4ea4\u6362\u6a21\u578b\u65b9\u9762\u3002</p>"},{"location":"LLM4phys/trans4phys/#equivariant-transformer-is-all-you-need","title":"Equivariant Transformer is all you need","text":"<ul> <li>https://arxiv.org/abs/2310.13222</li> </ul> <p>\u5728\u4e00\u4efd\u5f00\u521b\u6027\u7684\u8bba\u6587\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u79cb\u672c\u662d\u592b\u548c\u6c38\u4e95\u4f18\u5e0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u8ba1\u7b97\u7269\u7406\u5b66\u6982\u5ff5\u76f8\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\u3002\u4ed6\u4eec\u7684\u8bba\u6587\u300a\u7b49\u53d8 Transformer \u662f\u4f60\u6240\u9700\u8981\u7684\u300b\u4ecb\u7ecd\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86 Transformer \u795e\u7ecf\u7f51\u7edc\u7684\u529b\u91cf\u4e0e\u7269\u7406\u5bf9\u79f0\u6027\u539f\u7406\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u62df\u6548\u7387\u3002</p> <p>\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u6b63\u5728\u52a0\u901f\u8ba1\u7b97\u7269\u7406\u5b66\u7684\u53d1\u5c55\uff0c\u8fd9\u5df2\u88ab\u7528\u4e8e\u6a21\u62df\u6676\u683c\u4e0a\u7684\u7cfb\u7edf\u3002\u7b49\u53d8\u6027\u5bf9\u4e8e\u6a21\u62df\u7269\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63cf\u8ff0\u7684\u6982\u7387\u5206\u5e03\u65bd\u52a0\u4e86\u5f3a\u70c8\u7684\u5f52\u7eb3\u504f\u5dee\u3002\u8fd9\u964d\u4f4e\u4e86\u56e0\u504f\u79bb\u6570\u636e\u5bf9\u79f0\u6027\u548c\u7269\u7406\u5b9a\u5f8b\u800c\u5bfc\u81f4\u7684\u9519\u8bef\u5916\u63a8\u7684\u98ce\u9669\u3002\u7136\u800c\uff0c\u5728\u81ea\u5b66\u4e60\u8499\u7279\u5361\u7f57\uff08SLMC\uff09\u4e2d\u5f3a\u52a0\u5bf9\u79f0\u6027\u6709\u65f6\u4f1a\u5bfc\u81f4\u4f4e\u63a5\u53d7\u7387\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5728 GPT \u7b49 Transformer \u4e2d\u4f7f\u7528\u7684\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u5927\u6a21\u578b\u5bb9\u91cf\u3002\u6211\u4eec\u5f15\u5165\u4e86\u5bf9\u79f0\u7b49\u53d8\u6ce8\u610f\u529b\u5230 SLMC \u4e2d\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u67b6\u6784\uff0c\u6211\u4eec\u5c06\u5b83\u5e94\u7528\u4e8e\u6211\u4eec\u63d0\u51fa\u7684\u4e8c\u7ef4\u6676\u683c\u4e0a\u7684\u81ea\u65cb\u8d39\u7c73\u5b50\u6a21\u578b\u7684\u65b0\u67b6\u6784\u3002\u6211\u4eec\u53d1\u73b0\u5b83\u514b\u670d\u4e86\u7ebf\u6027\u6a21\u578b\u7684\u4f4e\u63a5\u53d7\u7387\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u63a5\u53d7\u7387\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5c31\u50cf\u5728\u5177\u6709 Transformer \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e00\u6837\u3002</p>"},{"location":"LLM4phys/trans4phys/#transformers-for-modeling-physical-systems","title":"Transformers for modeling physical systems","text":"<ul> <li>https://arxiv.org/abs/2010.03957</li> <li>https://github.com/zabaras/transformer-physx</li> </ul> <p>\u7269\u7406\u7cfb\u7edf\u7684\u5efa\u6a21\u4f20\u7edf\u4e0a\u4f9d\u8d56\u4e8e\u6570\u503c\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\uff0c\u8fd9\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u80fd\u591f\u9ad8\u6548\u8fd1\u4f3c\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4ee3\u7406\u6a21\u578b\u5df2\u6210\u4e3a\u9700\u8981\u5927\u91cf\u6a21\u62df\u7684\u5e94\u7528\u7684\u5173\u952e\u5de5\u5177\u3002\u5c3c\u53e4\u62c9\u65af\u00b7\u65e5\u5185\u74e6\u548c\u5c3c\u53e4\u62c9\u65af\u00b7\u624e\u5df4\u62c9\u65af\u7684\u8bba\u6587\u300a\u7528\u4e8e\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u7684 Transformer\u300b\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u521d\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5f00\u53d1\u7684 Transformer \u6a21\u578b\u5e94\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u7684\u52a8\u529b\u5b66\u7269\u7406\u7cfb\u7edf\u3002</p> <p>\u8fd9\u9879\u7814\u7a76\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u9886\u57df\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5f25\u5408\u4e86\u64c5\u957f\u6355\u6349\u5e8f\u5217\u6570\u636e\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u7684 Transformer \u67b6\u6784\u4e0e\u6a21\u62df\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u7269\u7406\u73b0\u8c61\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u4e0e\u4e4b\u524d\u91c7\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u6216\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u4e86 Transformer \u7684\u5e76\u884c\u5904\u7406\u80fd\u529b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u66f4\u6709\u6548\u5730\u6a21\u62df\u7269\u7406\u52a8\u529b\u5b66\u3002</p>"},{"location":"LLM4phys/trans4phys/#_2","title":"\u5b9e\u73b0\u65b9\u6cd5","text":"<p>\u4e3a\u4e86\u4f7f\u6b64\u7c7b\u52a8\u529b\u7cfb\u7edf\u7684\u5efa\u6a21\u9002\u7528\u4e8e\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u67b6\u6784\u7684\u4f7f\u7528\uff0c\u8fde\u7eed\u89e3\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u4e0a\u88ab\u79bb\u6563\u5316\uff0c\u4f7f\u5f97\u5fae\u5206\u65b9\u7a0b\u7684\u89e3\u4e3a $\\Phi = \\left( \\phi_0, \\phi_1, \\ldots, \\phi_T \\right)$\uff1b\u5176\u4e2d $\\phi_i \\in \\mathbb{R}^{n \\times d}$\uff0c\u4e14 $\\phi_i$ \u5728 $\\Omega$ \u4e0a\u88ab $d$ \u4e2a\u70b9\u79bb\u6563\u5316\u3002\u6211\u4eec\u5047\u8bbe\u521d\u59cb\u72b6\u6001\u4e3a $\\phi_0$\uff0c\u5e76\u4e14\u65f6\u95f4\u533a\u95f4 $\\mathcal{T}$ \u88ab $T$ \u4e2a\u65f6\u95f4\u6b65\u79bb\u6563\u5316\uff0c\u65f6\u95f4\u6b65\u957f\u4e3a $\\Delta t$\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u52a8\u529b\u7cfb\u7edf\u7684\u5efa\u6a21\u95ee\u9898\u8f6c\u5316\u4e3a\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u95ee\u9898\u3002\u6240\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6709\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7528\u4e8e\u5efa\u6a21\u52a8\u529b\u5b66\u7684Transformer\u548c\u7528\u4e8e\u5c06\u7269\u7406\u72b6\u6001\u6295\u5f71\u4e3a\u5411\u91cf\u8868\u793a\u7684\u5d4c\u5165\u7f51\u7edc\u3002\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7c7b\u4f3c\uff0c\u5d4c\u5165\u6a21\u578b\u5728Transformer\u4e4b\u524d\u8fdb\u884c\u8bad\u7ec3\u3002\u7136\u540e\uff0c\u8be5\u5d4c\u5165\u6a21\u578b\u88ab\u51bb\u7ed3\uff0c\u6574\u4e2a\u6570\u636e\u96c6\u88ab\u8f6c\u6362\u4e3a\u5d4c\u5165\u7a7a\u95f4\uff0c\u968f\u540e\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bad\u7ec3Transformer\uff0c\u5982\u56fe1\u6240\u793a\u3002\u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u5d4c\u5165\u89e3\u7801\u5668\u7528\u4e8e\u4eceTransformer\u7684\u9884\u6d4b\u4e2d\u91cd\u5efa\u7269\u7406\u72b6\u6001\u3002</p> <p>{width=80%}</p> <p>\u56fe 2: \u4f7f\u7528\u53d8\u538b\u5668\u5efa\u6a21\u7269\u7406\u52a8\u529b\u5b66\u7684\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u3002\uff08\u4ece\u5de6\u5230\u53f3\uff09\u9996\u5148\u4f7f\u7528 Koopman \u52a8\u529b\u5b66\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u3002\u7136\u540e\u51bb\u7ed3\uff08\u56fa\u5b9a\uff09\u5d4c\u5165\u6a21\u578b\uff0c\u5c06\u6240\u6709\u8bad\u7ec3\u6570\u636e\u5d4c\u5165\uff0c\u5e76\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bad\u7ec3\u53d8\u538b\u5668\u3002</p>"},{"location":"LLM4phys/trans4phys/#transformer_2","title":"\u4f7f\u7528 Transformer \u6765\u5b66\u4e60\u5206\u5b50\u548c\u86cb\u767d\u8d28\u7684\u8bed\u8a00\u7684\u6848\u4f8b","text":"<ul> <li>https://github.com/HICAI-ZJU/Scientific-LLM-Survey</li> </ul>"},{"location":"LLM4phys/trans4phys/#scientific-large-language-models-a-survey-on-biological-chemical-domains","title":"Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains","text":"<p>https://arxiv.org/abs/2401.14656</p> <p>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fd1\u671f\u8fdb\u5c55\u5df2\u7ecf\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u5728\u7279\u5b9a\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u751f\u7269\u5b66\u548c\u5316\u5b66\u9886\u57df\uff0c\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\u548c\u673a\u9047\u3002\u79d1\u5b66\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Sci-LLMs\uff09\u65e8\u5728\u7406\u89e3\u548c\u751f\u6210\u79d1\u5b66\u8bed\u8a00\uff0c\u8303\u56f4\u4ece\u4e13\u4e1a\u7b26\u53f7\u7cfb\u7edf\u5230\u590d\u6742\u7684\u5206\u5b50\u7ed3\u6784\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u3002</p> <p>\u5982\u56fe\u6240\u793a\uff0c\u79d1\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u6a21\u6001\u4e0a\u8fd0\u884c\uff0c\u5305\u62ec\u6587\u672c\u3001\u5206\u5b50\u3001\u86cb\u767d\u8d28\u548c\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u4f4d\u4e8e\u8fd9\u4e9b\u9886\u57df\u7684\u4ea4\u6c47\u5904\u3002\u4e0e\u4e3b\u8981\u5728\u81ea\u7136\u8bed\u8a00\u6587\u672c\u4e0a\u8bad\u7ec3\u7684\u901a\u7528\u76ee\u7684\u6a21\u578b\u4e0d\u540c\uff0c\u79d1\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u5fc5\u987b\u7406\u89e3\u79d1\u5b66\u7684\u72ec\u7279\u201c\u8bed\u8a00\u201d\uff0c\u5305\u62ec\u4e13\u95e8\u7684\u7b26\u53f7\u7cfb\u7edf\uff08\u5982\u5206\u5b50\u7684 SMILES\uff09\u548c\u751f\u7269\u5e8f\u5217\u4e2d\u53d1\u73b0\u7684\u590d\u6742\u6a21\u5f0f\u3002</p> <p>{width=80%}</p> <p>\u56fe 3: \u672c\u8c03\u67e5\u4e2d\u79d1\u5b66\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Sci-LLMs\uff09\u7684\u7814\u7a76\u8303\u56f4\u3002\u6211\u4eec\u5173\u6ce8\u79d1\u5b66\u8bed\u8a00\uff08\u5373\u6587\u672c\u3001\u5206\u5b50\u3001\u86cb\u767d\u8d28\u548c\u57fa\u56e0\u7ec4\u8bed\u8a00\uff09\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u751f\u5316\u79d1\u5b66\u9886\u57df\u7684\u7ec4\u5408\uff08\u5373\u591a\u6a21\u6001\u8bed\u8a00\uff09\u3002</p>"},{"location":"LLM4phys/trans4phys/#_3","title":"\u6838\u5fc3\u89c2\u70b9","text":"<ul> <li>\u901a\u7528LLMs\u53ea\u80fd\u5904\u7406\u81ea\u7136\u8bed\u8a00\uff0c\u65e0\u6cd5\u59a5\u5584\u5904\u7406\u5206\u5b50\u548c\u86cb\u767d\u8d28\u7b49\u79d1\u5b66\u6570\u636e\u3002</li> </ul> <p>\u9664\u4e86\u81ea\u7136\u8bed\u8a00\u4e4b\u5916\uff0c\u4e3a\u4e86\u5c01\u88c5\u66f4\u591a\u7684\u4e13\u95e8\u79d1\u5b66\u77e5\u8bc6\uff0c\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u79d1\u5b66\u8bed\u8a00\uff0c\u5982\u56fe 1 \u6240\u793a\u3002\u8fd9\u5305\u62ec\u79d1\u5b66\u7814\u7a76\u9886\u57df\u7684\u6587\u672c\u8868\u8fbe\u3001\u5b9a\u4e49\u6570\u5b66\u516c\u5f0f\u7684\u6570\u5b66\u8bed\u8a00\u3001\u8868\u793a\u5206\u5b50\u7ed3\u6784\u7684\u5316\u5b66\u8bed\u8a00\uff08\u5982 SMILES\uff09\u4ee5\u53ca\u63cf\u8ff0\u86cb\u767d\u8d28\u6216\u57fa\u56e0\u7ec4\u7684\u751f\u7269\u8bed\u8a00\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u751f\u7269\u4f53\u7684\u590d\u6742\u6784\u6210\u3002\u8fd9\u4e9b\u79d1\u5b66\u8bed\u8a00\u6709\u81ea\u5df1\u7684\u8bcd\u6c47\u8868\uff0c\u5176\u4e2d\u6bcf\u4e2a\u672f\u8bed\u90fd\u6709\u7279\u5b9a\u7684\u542b\u4e49\uff0c\u53ef\u80fd\u4e0e\u81ea\u7136\u8bed\u8a00\u5b8c\u5168\u4e0d\u540c\u3002\u4f8b\u5982\uff0c\u5728\u82f1\u8bed\u4e2d\uff0c\u201cC\u201d\u5b57\u7b26\u5728\u86cb\u767d\u8d28\u8bed\u8a00\u4e2d\u4ee3\u8868\u534a\u80f1\u6c28\u9178\uff08GDR \u7b49\uff0c1984\uff09\uff0c\u800c\u5728 SMILES \u8bed\u8a00\u7cfb\u7edf\u4e2d\u5219\u8868\u793a\u78b3\u539f\u5b50\uff08Weininger\uff0c1988\uff09\u3002\u6b64\u5916\uff0c\u7279\u5b9a\u9886\u57df\u7684\u4e13\u5bb6\u5236\u5b9a\u4e86\u7ec4\u7ec7\u8fd9\u4e9b\u672f\u8bed\u7684\u8bed\u6cd5\u89c4\u5219\uff0c\u4ece\u800c\u6784\u5efa\u5177\u6709\u7cbe\u786e\u8bed\u4e49\u529f\u80fd\u7684\u53e5\u5b50\u3002\u4f8b\u5982\uff0c\u8ba1\u7b97\u5316\u5b66\u5bb6\u5236\u5b9a\u4e86\u8bed\u6cd5\u89c4\u5219\uff0c\u4ee5\u786e\u4fdd SELFIES \u683c\u5f0f\u7684\u673a\u5668\u751f\u6210\u5206\u5b50\u7684\u51c6\u786e\u6027\uff08Krenn \u7b49\uff0c2020\uff09\u3002 \u7ecf\u8fc7\u51e0\u5341\u5e74\u7684\u53d1\u5c55\uff0c\u79d1\u5b66\u8bed\u8a00\u5df2\u6210\u4e3a\u4e0d\u53ef\u6216\u7f3a\u7684\u5de5\u5177\uff0c\u6781\u5927\u5730\u52a0\u901f\u4e86\u79d1\u5b66\u53d1\u73b0\u3002\u7531\u4e8e\u79d1\u5b66\u8bed\u8a00\u548c\u81ea\u7136\u8bed\u8a00\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u8bed\u4e49\u548c\u8bed\u6cd5\u5dee\u5f02\uff0c\u73b0\u6709\u7684\u901a\u7528LLMs\uff08\u5982 ChatGPT 1 \u6216 GPT-4 (OpenAI, 2023)\uff09\u5f80\u5f80\u65e0\u6cd5\u59a5\u5584\u5904\u7406\u5206\u5b50\u548c\u86cb\u767d\u8d28\u7b49\u79d1\u5b66\u6570\u636e\uff08AI4Science and Quantum, 2023\uff09\u3002\u6b63\u5982\u8457\u540d\u7684\u5965\u5730\u5229\u54f2\u5b66\u5bb6\u8def\u5fb7\u7ef4\u5e0c\u00b7\u7ef4\u7279\u6839\u65af\u5766\u6240\u6307\u51fa\u7684\uff0c\u201c\u6211\u7684\u8bed\u8a00\u7684\u754c\u9650\u610f\u5473\u7740\u6211\u7684\u4e16\u754c\u7684\u754c\u9650\u3002\u201d\uff08Ramsey, 1923\uff09\u901a\u7528LLMs\u7684\u4e16\u754c\u53ef\u80fd\u5c40\u9650\u4e8e\u81ea\u7136\u8bed\u8a00\u3002</p>"},{"location":"LLMdeploy/LLMdeploy/","title":"\u672c\u5730\u90e8\u7f72LLM","text":"<ul> <li>\u7ed9 LLM \u6295\u5582\u7269\u7406\u77e5\u8bc6</li> <li>\u5e38\u7528\u7684\u65b9\u6848</li> <li>\u53ef\u884c\u7684\u65b9\u6848<ul> <li>GraphRAG</li> <li>Fine-tuning</li> </ul> </li> <li>CCNU NSC3 \u96c6\u7fa4 vLLM \u672c\u5730\u90e8\u7f72 DeepSeek-R1</li> <li>vLLM \u7b80\u4ecb<ul> <li>\u6838\u5fc3\u7279\u6027</li> <li>\u5b89\u88c5\u6761\u4ef6</li> </ul> </li> <li>vLLM \u5b89\u88c5\u65b9\u5f0f\u4e00\uff1a\u901a\u8fc7\u642d\u5efa python \u865a\u62df\u73af\u5883\u5b89\u88c5 vLLM<ul> <li>\u73af\u5883\u51c6\u5907</li> <li>\u6a21\u578b\u4e0b\u8f7d</li> <li>\u4ee3\u7801\u51c6\u5907</li> <li>Python\u811a\u672c</li> <li>\u521b\u5efa\u517c\u5bb9 OpenAI API \u63a5\u53e3\u7684\u670d\u52a1\u5668</li> <li>\u63a8\u7406\u901f\u5ea6\u6d4b\u8bd5</li> <li>vLLM Reference</li> </ul> </li> <li>vLLM/SGLang \u5b89\u88c5\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u62c9\u53d6\u5b98\u65b9\u7684 Docker \u5bb9\u5668\u5b89\u88c5</li> <li>sglang \u53cc\u8282\u70b9\u90e8\u7f72</li> <li>\u8282\u70b91 gpu039</li> <li>\u8282\u70b92 gpu040</li> <li>sglang \u5b98\u65b9</li> <li>\u5355\u4e00\u8282\u70b9</li> <li>API\u670d\u52a1\u90e8\u7f72\u65b9\u5f0f</li> <li>open-webui \u90e8\u7f72</li> <li>curl</li> </ul>"},{"location":"LLMdeploy/LLMdeploy/#llm","title":"\u7ed9 LLM \u6295\u5582\u7269\u7406\u77e5\u8bc6","text":""},{"location":"LLMdeploy/LLMdeploy/#_1","title":"\u5e38\u7528\u7684\u65b9\u6848","text":"<p>\u7ed9\u5927\u8bed\u8a00\u6a21\u578b\u6295\u5582 Lattice QCD \u77e5\u8bc6\u5e93\uff0c\u901a\u5e38\u6709\u5982\u4e0b\u4e09\u79cd\u65b9\u5f0f\uff1a\u63d0\u793a\u8bcd\uff08Prompt\uff09\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Retrieval-Augmented Generation\uff0cRAG\uff09\uff0c\u5fae\u8c03\uff08Fine-tuning\uff09\u3002</p> <p>\u4e09\u79cd\u65b9\u5f0f\u7684\u5b9e\u73b0\u539f\u7406\u5bf9\u6bd4\uff1a </p> <p>\u4e09\u79cd\u65b9\u5f0f\u7684\u4f18\u7f3a\u70b9\u5bf9\u6bd4\uff1a </p> <p>\u56fe\u8868\u6765\u81ea\uff1a \u3010\u5982\u4f55\u7ed9\u5927\u6a21\u578b\u5582\u6570\u636e\uff1f\u8ba9AI\u66f4\u61c2\u4f60\uff5e\u3010\u5c0f\u767d\u79d1\u666e\u3011\u3011 https://www.bilibili.com/video/BV1HS421R7oL/?share_source=copy_web&amp;vd_source=4b438f829d0c01700eb6160fae7d5ea7</p>"},{"location":"LLMdeploy/LLMdeploy/#_2","title":"\u53ef\u884c\u7684\u65b9\u6848","text":"<p>\u63d0\u793a\u8bcd\u7684\u65b9\u5f0f\u901a\u5e38\u53ea\u652f\u6301128k\u4e0a\u4e0b\u6587\uff0c\u663e\u7136\u6ca1\u6cd5\u80dc\u4efb\u6d77\u91cf\u7684\u77e5\u8bc6\u5e93\uff0c\u5269\u4e0b\u4e24\u79cd\u65b9\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5fae\u8c03\u5012\u662f\u53ef\u884c\u7684\u3002</p>"},{"location":"LLMdeploy/LLMdeploy/#graphrag","title":"GraphRAG","text":"<p>\u672c\u5730\u5b89\u88c5\u5fae\u8f6f\u5f00\u6e90 GraphRAG\u3002GraphRAG\u4f18\u4e8e\u4f20\u7edf\u7684RAG\uff0c\u5e94\u8be5\u80fd\u591f\u80dc\u4efb\u5bf9\u683c\u70b9QCD\u77e5\u8bc6\u7684\u6574\u7406\u3002</p> <ul> <li>\u4f18\u70b9\u662f\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528deepseek R1\u7684API\uff0c\u6027\u80fd\u5f3a\u52b2\u4e14\u4fbf\u5b9c\u3002</li> <li>\u7f3a\u70b9\u662f\u5fae\u8f6f\u7684GraphRAG\u53ea\u652f\u6301txt\u548ccsv\u7684\u6587\u4ef6\u683c\u5f0f\u3002\u5bf9\u4e8e\u516c\u5f0f\u4e5f\u662f\u5f53\u4f5c\u6587\u672c\u6765\u5904\u7406\uff0c\u56fe\u7247\u5e76\u4e0d\u652f\u6301\uff0c\u9700\u8981\u9884\u5148\u5904\u7406\uff0c\u63d0\u53d6\u51fa\u56fe\u7247\u4e2d\u7684\u6587\u672c\u4fe1\u606f\u3002</li> </ul>"},{"location":"LLMdeploy/LLMdeploy/#fine-tuning","title":"Fine-tuning","text":"<p>\u5fae\u8c03\u6700\u5927\u7684\u56f0\u96be\u5728\u4e8e\u6570\u636e\u51c6\u5907\uff0c\u6b65\u9aa4\u5982\u4e0b\uff1a 1. \u6536\u96c6\u7269\u7406\u9886\u57df\u6570\u636e\uff1a\u6536\u96c6\u4e0e\u7279\u5b9a\u7269\u7406\u9886\u57df\u76f8\u5173\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5982\u7269\u7406\u5b66\u672f\u8bba\u6587\u3001\u6559\u79d1\u4e66\u3001\u5b9e\u9a8c\u62a5\u544a\u7b49\u3002\u786e\u4fdd\u6570\u636e\u6db5\u76d6\u76ee\u6807\u7269\u7406\u9886\u57df\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u672f\u8bed\u3002 2. \u6570\u636e\u6e05\u6d17\u548c\u6807\u6ce8\uff1a\u5bf9\u6536\u96c6\u5230\u7684\u6570\u636e\u8fdb\u884c\u6e05\u6d17\uff0c\u53bb\u9664\u566a\u58f0\u548c\u4e0d\u76f8\u5173\u7684\u4fe1\u606f\u3002\u6839\u636e\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u4f8b\u5982\u6807\u6ce8\u7269\u7406\u516c\u5f0f\u3001\u6982\u5ff5\u89e3\u91ca\u3001\u5b9e\u9a8c\u6b65\u9aa4\u7b49\u3002 3. \u6570\u636e\u683c\u5f0f\u5316\uff1a\u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u6a21\u578b\u53ef\u63a5\u53d7\u7684\u683c\u5f0f\uff0c\u5982JSONL\u6587\u4ef6\uff0c\u5305\u542b\u6210\u5bf9\u7684\u95ee\u9898\u548c\u56de\u7b54\u3002</p> <p>\u51c6\u5907\u597d\u4e86\u7528\u4e8e\u5fae\u8c03\u7684\u6570\u636e\uff0c\u8fd8\u8981\u5728\u672c\u5730\u90e8\u7f72deepseek R1\uff0c\u80fd\u591f\u672c\u5730\u90e8\u7f72\u7684\u90fd\u662fQwen\u6a21\u578b\u84b8\u998fdeepseek R1\u5f97\u5230\u7684\uff0c\u6027\u80fd\u8fdc\u4e0d\u5982deepseek R1\u3002</p>"},{"location":"LLMdeploy/LLMdeploy/#ccnu-nsc3-vllm-deepseek-r1","title":"CCNU NSC3 \u96c6\u7fa4 vLLM \u672c\u5730\u90e8\u7f72 DeepSeek-R1","text":""},{"location":"LLMdeploy/LLMdeploy/#vllm","title":"vLLM \u7b80\u4ecb","text":"<p>vLLM \u662f\u4f2f\u514b\u5229\u5927\u5b66 LMSYS \u7ec4\u7ec7\u5f00\u6e90\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u901f\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u6781\u5927\u5730\u63d0\u5347\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u541e\u5410\u91cf\u4e0e\u5185\u5b58\u4f7f\u7528\u6548\u7387\u3002</p>"},{"location":"LLMdeploy/LLMdeploy/#_3","title":"\u6838\u5fc3\u7279\u6027","text":"<ul> <li>\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\uff1a\u901a\u8fc7 <code>PagedAttention</code> \u7b97\u6cd5\uff0c<code>vLLM</code> \u5b9e\u73b0\u4e86\u5bf9 <code>KV</code> \u7f13\u5b58\u7684\u9ad8\u6548\u7ba1\u7406\uff0c\u51cf\u5c11\u4e86\u5185\u5b58\u6d6a\u8d39\uff0c\u4f18\u5316\u4e86\u6a21\u578b\u7684\u8fd0\u884c\u6548\u7387\u3002</li> <li>\u9ad8\u541e\u5410\u91cf\uff1a<code>vLLM</code> \u652f\u6301\u5f02\u6b65\u5904\u7406\u548c\u8fde\u7eed\u6279\u5904\u7406\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u63a8\u7406\u7684\u541e\u5410\u91cf\uff0c\u52a0\u901f\u4e86\u6587\u672c\u751f\u6210\u548c\u5904\u7406\u901f\u5ea6\u3002\u4e0e Hugging Face Transformers \u76f8\u6bd4\uff0cvLLM \u7684\u541e\u5410\u91cf\u6700\u591a\u53ef\u4ee5\u8fbe\u5230\u5176 24 \u500d\uff0c\u6587\u672c\u751f\u6210\u63a8\u7406\uff08TGI\uff09\u9ad8\u51fa 3.5 \u500d\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5bf9\u6a21\u578b\u7ed3\u6784\u8fdb\u884c\u4efb\u4f55\u7684\u6539\u53d8\u3002</li> <li>\u5185\u5b58\u6548\u7387\uff1avLLM \u5b9e\u73b0\u4e86 KV \u7f13\u5b58\u5185\u5b58\u51e0\u4e4e\u96f6\u6d6a\u8d39\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u74f6\u9888\u95ee\u9898\u3002</li> <li>\u786c\u4ef6\u517c\u5bb9\u6027\uff1avLLM \u4e0d\u4ec5\u652f\u6301 NVIDIA GPU\uff0c\u8fd8\u5bf9 AMD GPU\u3001Intel GPU\u3001AWS Neuron \u548c Google TPU \u7b49\u5e02\u9762\u4e0a\u4f17\u591a\u786c\u4ef6\u67b6\u6784\u655e\u5f00\u6000\u62b1\u3002</li> <li>\u6613\u7528\u6027\uff1a<code>vLLM</code> \u4e0e <code>HuggingFace</code> \u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u591a\u79cd\u6d41\u884c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7b80\u5316\u4e86\u6a21\u578b\u90e8\u7f72\u548c\u63a8\u7406\u7684\u8fc7\u7a0b\u3002\u517c\u5bb9 <code>OpenAI</code> \u7684 <code>API</code> \u670d\u52a1\u5668\u3002</li> <li>\u5206\u5e03\u5f0f\u63a8\u7406\uff1a\u6846\u67b6\u652f\u6301\u5728\u591a <code>GPU</code> \u73af\u5883\u4e2d\u8fdb\u884c\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u901a\u8fc7\u6a21\u578b\u5e76\u884c\u7b56\u7565\u548c\u9ad8\u6548\u7684\u6570\u636e\u901a\u4fe1\uff0c\u63d0\u5347\u4e86\u5904\u7406\u5927\u578b\u6a21\u578b\u7684\u80fd\u529b\u3002</li> <li>\u591a\u6b65\u8c03\u5ea6\uff1avLLM \u5f15\u5165\u4e86\u591a\u6b65\u8c03\u5ea6\u6280\u672f\uff0c\u5141\u8bb8\u4e00\u6b21\u6027\u5b8c\u6210\u591a\u4e2a\u6b65\u9aa4\u7684\u8c03\u5ea6\u548c\u8f93\u5165\u51c6\u5907\uff0c\u4f7f\u5f97 GPU \u53ef\u4ee5\u8fde\u7eed\u5904\u7406\u591a\u4e2a\u6b65\u9aa4\u800c\u4e0d\u5fc5\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u7b49\u5f85 CPU \u6307\u4ee4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86 GPU \u7684\u5229\u7528\u7387\u548c\u6574\u4f53\u541e\u5410\u91cf\u3002</li> <li>\u5f02\u6b65\u8f93\u51fa\u5904\u7406\uff1avLLM \u91c7\u7528\u5f02\u6b65\u8f93\u51fa\u5904\u7406\u6280\u672f\uff0c\u4f7f\u5f97\u8f93\u51fa\u5904\u7406\u4e0e\u6a21\u578b\u7684\u6267\u884c\u53ef\u4ee5\u5e76\u884c\u8fdb\u884c\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5904\u7406\u6548\u7387\u3002</li> <li>\u6a21\u578b\u5fae\u8c03\uff1avLLM \u652f\u6301\u4f7f\u7528 LoRA\uff08Low-Rank Adaptation\uff09\u6280\u672f\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u65b9\u6cd5\u3002</li> <li>\u5f00\u6e90\u5171\u4eab\uff1a<code>vLLM</code> \u7531\u4e8e\u5176\u5f00\u6e90\u7684\u5c5e\u6027\uff0c\u62e5\u6709\u6d3b\u8dc3\u7684\u793e\u533a\u652f\u6301\uff0c\u8fd9\u4e5f\u4fbf\u4e8e\u5f00\u53d1\u8005\u8d21\u732e\u548c\u6539\u8fdb\uff0c\u5171\u540c\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u3002</li> </ul>"},{"location":"LLMdeploy/LLMdeploy/#_4","title":"\u5b89\u88c5\u6761\u4ef6","text":"<ul> <li>vLLM \u652f\u6301\u5728 Linux \u7cfb\u7edf\u4e0a\u8fd0\u884c\u3002</li> <li>\u9700\u8981 Python 3.8-3.11 \u7248\u672c\uff0cCUDA 12.1\u3002</li> <li>\u9700\u8981\u5177\u6709\u8ba1\u7b97\u80fd\u529b 7.0 \u6216\u66f4\u9ad8\u7684 GPU\uff08\u5982 V100\u3001T4\u3001RTX 20xx\u3001A100\u3001L4\u3001H100 \u7b49\uff09\u3002</li> </ul>"},{"location":"LLMdeploy/LLMdeploy/#vllm-python-vllm","title":"vLLM \u5b89\u88c5\u65b9\u5f0f\u4e00\uff1a\u901a\u8fc7\u642d\u5efa python \u865a\u62df\u73af\u5883\u5b89\u88c5 vLLM","text":""},{"location":"LLMdeploy/LLMdeploy/#_5","title":"\u73af\u5883\u51c6\u5907","text":"<p>\u672c\u6587\u57fa\u7840\u73af\u5883\u5982\u4e0b\uff1a</p> <pre><code>----------------\nCentos 7.7\npython 3.12\ncuda 12.4\nCUDA DRIVER 550.54.14\npytorch 2.5.1\ngcc 11\ng++ 11\n----------------\n</code></pre> <pre><code># make a virtual environment first\nconda create -n cuda124_vllm python=3.12 -y\nmodule load cuda-12.4\nconda activate cuda124_vllm\nconda install -c conda-forge gcc=11\nconda install -c conda-forge gxx=11\npip install torch\npip install vllm\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#_6","title":"\u6a21\u578b\u4e0b\u8f7d","text":"<p>\u4f7f\u7528 modelscope \u4e2d\u7684 snapshot_download \u51fd\u6570\u4e0b\u8f7d\u6a21\u578b\uff0c\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e3a\u6a21\u578b\u540d\u79f0\uff0c\u53c2\u6570 cache_dir \u4e3a\u6a21\u578b\u7684\u4e0b\u8f7d\u8def\u5f84\u3002</p> <p>\u65b0\u5efa <code>model_download.py</code> \u6587\u4ef6\u5e76\u5728\u5176\u4e2d\u8f93\u5165\u4ee5\u4e0b\u5185\u5bb9\uff0c\u7c98\u8d34\u4ee3\u7801\u540e\u8bb0\u5f97\u4fdd\u5b58\u6587\u4ef6\u3002</p> <pre><code>from modelscope import snapshot_download\n\nmodel_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', cache_dir='/dssg/work/kfye/deepseek', revision='master')\n</code></pre> <p>\u7136\u540e\u5728\u7ec8\u7aef\u4e2d\u8f93\u5165 <code>python model_download.py</code> \u6267\u884c\u4e0b\u8f7d\uff0c\u8fd9\u91cc\u9700\u8981\u8010\u5fc3\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u76f4\u5230\u6a21\u578b\u4e0b\u8f7d\u5b8c\u6210\u3002</p> <p>\u6ce8\u610f\uff1a\u8bb0\u5f97\u4fee\u6539 <code>cache_dir</code> \u4e3a\u4f60\u7684\u6a21\u578b\u4e0b\u8f7d\u8def\u5f84\u54e6~</p>"},{"location":"LLMdeploy/LLMdeploy/#_7","title":"\u4ee3\u7801\u51c6\u5907","text":""},{"location":"LLMdeploy/LLMdeploy/#python","title":"Python\u811a\u672c","text":"<p>\u65b0\u5efa <code>vllm_model.py</code> \u6587\u4ef6\u5e76\u5728\u5176\u4e2d\u8f93\u5165\u4ee5\u4e0b\u5185\u5bb9\uff0c\u7c98\u8d34\u4ee3\u7801\u540e\u8bf7\u53ca\u65f6\u4fdd\u5b58\u6587\u4ef6\u3002\u4e0b\u9762\u7684\u4ee3\u7801\u6709\u5f88\u8be6\u7ec6\u7684\u6ce8\u91ca\uff0c\u5982\u6709\u4e0d\u7406\u89e3\u7684\u5730\u65b9\uff0c\u6b22\u8fce\u5927\u5bb6\u63d0 <code>issue</code>\u3002</p> <p>\u9996\u5148\u4ece <code>vLLM</code> \u5e93\u4e2d\u5bfc\u5165 <code>LLM</code> \u548c <code>SamplingParams</code> \u7c7b\u3002<code>LLM</code> \u7c7b\u662f\u4f7f\u7528 <code>vLLM</code> \u5f15\u64ce\u8fd0\u884c\u79bb\u7ebf\u63a8\u7406\u7684\u4e3b\u8981\u7c7b\u3002<code>SamplingParams</code> \u7c7b\u6307\u5b9a\u91c7\u6837\u8fc7\u7a0b\u7684\u53c2\u6570\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u8c03\u6574\u751f\u6210\u6587\u672c\u7684\u968f\u673a\u6027\u548c\u591a\u6837\u6027\u3002</p> <p><code>vLLM</code> \u63d0\u4f9b\u4e86\u975e\u5e38\u65b9\u4fbf\u7684\u5c01\u88c5\uff0c\u6211\u4eec\u76f4\u63a5\u4f20\u5165\u6a21\u578b\u540d\u79f0\u6216\u6a21\u578b\u8def\u5f84\u5373\u53ef\uff0c\u4e0d\u5fc5\u624b\u52a8\u521d\u59cb\u5316\u6a21\u578b\u548c\u5206\u8bcd\u5668\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u4ee3\u7801\u793a\u4f8b\u719f\u6089\u4e0b <code>vLLM</code> \u5f15\u64ce\u7684\u4f7f\u7528\u65b9\u5f0f\u3002\u88ab\u6ce8\u91ca\u7684\u90e8\u5206\u5185\u5bb9\u53ef\u4ee5\u4e30\u5bcc\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f46\u4e0d\u662f\u5fc5\u8981\u7684\uff0c\u5927\u5bb6\u53ef\u4ee5\u6309\u9700\u9009\u62e9\uff0c\u81ea\u5df1\u591a\u591a\u52a8\u624b\u5c1d\u8bd5 ~</p> <pre><code># vllm_model.py\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\nimport os\nimport json\n\n# \u81ea\u52a8\u4e0b\u8f7d\u6a21\u578b\u65f6\uff0c\u6307\u5b9a\u4f7f\u7528modelscope; \u5426\u5219\uff0c\u4f1a\u4eceHuggingFace\u4e0b\u8f7d\nos.environ['VLLM_USE_MODELSCOPE']='True'\n\ndef get_completion(prompts, model, tokenizer=None, max_tokens=8192, temperature=0.6, top_p=0.95, max_model_len=2048):\n    stop_token_ids = [151329, 151336, 151338]\n    # \u521b\u5efa\u91c7\u6837\u53c2\u6570\u3002temperature \u63a7\u5236\u751f\u6210\u6587\u672c\u7684\u591a\u6837\u6027\uff0ctop_p \u63a7\u5236\u6838\u5fc3\u91c7\u6837\u7684\u6982\u7387\n    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)\n    # \u521d\u59cb\u5316 vLLM \u63a8\u7406\u5f15\u64ce\n    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)\n    outputs = llm.generate(prompts, sampling_params)\n    return outputs\n\n\nif __name__ == \"__main__\":    \n    # \u521d\u59cb\u5316 vLLM \u63a8\u7406\u5f15\u64ce\n    model='/dssg/work/kfye/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' # \u6307\u5b9a\u6a21\u578b\u8def\u5f84\n    # model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # \u6307\u5b9a\u6a21\u578b\u540d\u79f0\uff0c\u81ea\u52a8\u4e0b\u8f7d\u6a21\u578b\n    tokenizer = None\n    # \u52a0\u8f7d\u5206\u8bcd\u5668\u540e\u4f20\u5165vLLM \u6a21\u578b\uff0c\u4f46\u4e0d\u662f\u5fc5\u8981\u7684\u3002\n    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) \n\n    text = [\"Please introduce Pade approximants for me. Please reason step by step, and put your final answer within \\boxed{}.&lt;think&gt;\\n\", ] # \u53ef\u7528 List \u540c\u65f6\u4f20\u5165\u591a\u4e2a prompt\uff0c\u6839\u636e DeepSeek \u5b98\u65b9\u7684\u5efa\u8bae\uff0c\u6bcf\u4e2a prompt \u90fd\u9700\u8981\u4ee5 &lt;think&gt;\\n \u7ed3\u5c3e\uff0c\u5982\u679c\u662f\u6570\u5b66\u63a8\u7406\u5185\u5bb9\uff0c\u5efa\u8bae\u5305\u542b\uff08\u4e2d\u82f1\u6587\u7686\u53ef\uff09\uff1aPlease reason step by step, and put your final answer within \\boxed{}.\n\n    # messages = [\n    #     {\"role\": \"user\", \"content\": prompt+\"&lt;think&gt;\\n\"}\n    # ]\n    # \u4f5c\u4e3a\u804a\u5929\u6a21\u677f\u7684\u6d88\u606f\uff0c\u4e0d\u662f\u5fc5\u8981\u7684\u3002\n    # text = tokenizer.apply_chat_template(\n    #     messages,\n    #     tokenize=False,\n    #     add_generation_prompt=True\n    # )\n\n    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=8192, temperature=0.6, top_p=0.95, max_model_len=2048) # \u601d\u8003\u9700\u8981\u8f93\u51fa\u66f4\u591a\u7684 Token \u6570\uff0cmax_tokens \u8bbe\u4e3a 8K\uff0c\u6839\u636e DeepSeek \u5b98\u65b9\u7684\u5efa\u8bae\uff0ctemperature\u5e94\u5728 0.5-0.7\uff0c\u63a8\u8350 0.6\n\n    # \u8f93\u51fa\u662f\u4e00\u4e2a\u5305\u542b prompt\u3001\u751f\u6210\u6587\u672c\u548c\u5176\u4ed6\u4fe1\u606f\u7684 RequestOutput \u5bf9\u8c61\u5217\u8868\u3002\n    # \u6253\u5370\u8f93\u51fa\u3002\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        if r\"&lt;/think&gt;\" in generated_text:\n            think_content, answer_content = generated_text.split(r\"&lt;/think&gt;\")\n        else:\n            think_content = \"\"\n            answer_content = generated_text\n        print(f\"Prompt: {prompt!r}, Think: {think_content!r}, Answer: {answer_content!r}\")\n</code></pre> <p>\u8fd0\u884c\u4ee3\u7801</p> <pre><code>python vllm_model.py\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#openai-api","title":"\u521b\u5efa\u517c\u5bb9 OpenAI API \u63a5\u53e3\u7684\u670d\u52a1\u5668","text":"<p><code>DeepSeek-R1-Distill-Qwen</code> \u517c\u5bb9 <code>OpenAI API</code> \u534f\u8bae\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>vLLM</code> \u521b\u5efa <code>OpenAI API</code> \u670d\u52a1\u5668\u3002<code>vLLM</code> \u90e8\u7f72\u5b9e\u73b0 <code>OpenAI API</code> \u534f\u8bae\u7684\u670d\u52a1\u5668\u975e\u5e38\u65b9\u4fbf\u3002\u9ed8\u8ba4\u4f1a\u5728 http://localhost:8000 \u542f\u52a8\u670d\u52a1\u5668\u3002\u670d\u52a1\u5668\u5f53\u524d\u4e00\u6b21\u6258\u7ba1\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u5217\u8868\u6a21\u578b\u3001<code>completions</code> \u548c <code>chat completions</code> \u7aef\u53e3\u3002</p> <ul> <li><code>completions</code>\uff1a\u662f\u57fa\u672c\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u6a21\u578b\u4f1a\u5728\u7ed9\u5b9a\u7684\u63d0\u793a\u540e\u751f\u6210\u4e00\u6bb5\u6587\u672c\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u4efb\u52a1\u901a\u5e38\u7528\u4e8e\u751f\u6210\u6587\u7ae0\u3001\u6545\u4e8b\u3001\u90ae\u4ef6\u7b49\u3002</li> <li><code>chat completions</code>\uff1a\u662f\u9762\u5411\u5bf9\u8bdd\u7684\u4efb\u52a1\uff0c\u6a21\u578b\u9700\u8981\u7406\u89e3\u548c\u751f\u6210\u5bf9\u8bdd\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u4efb\u52a1\u901a\u5e38\u7528\u4e8e\u6784\u5efa\u804a\u5929\u673a\u5668\u4eba\u6216\u8005\u5bf9\u8bdd\u7cfb\u7edf\u3002</li> </ul> <p>\u5728\u521b\u5efa\u670d\u52a1\u5668\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u3001\u6a21\u578b\u8def\u5f84\u3001\u804a\u5929\u6a21\u677f\u7b49\u53c2\u6570\u3002</p> <ul> <li><code>--host</code> \u548c <code>--port</code> \u53c2\u6570\u6307\u5b9a\u5730\u5740\u3002</li> <li><code>--model</code> \u53c2\u6570\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u3002</li> <li><code>--chat-template</code> \u53c2\u6570\u6307\u5b9a\u804a\u5929\u6a21\u677f\u3002</li> <li><code>--served-model-name</code> \u6307\u5b9a\u670d\u52a1\u6a21\u578b\u7684\u540d\u79f0\u3002</li> <li><code>--max-model-len</code> \u6307\u5b9a\u6a21\u578b\u7684\u6700\u5927\u957f\u5ea6\u3002</li> </ul> <pre><code>conda activate cuda124_vllm\npython -m vllm.entrypoints.openai.api_server --model /dssg/work/kfye/deepseek/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --served-model-name deepseekr1-qwen-1.5b --tensor-parallel-size 8 --max-model-len 2048\npython -m vllm.entrypoints.openai.api_server --model /dssg/work/kfye/deepseek/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --served-model-name deepseekr1-Llama-70b --tensor-parallel-size 8 --max-model-len 2048\n</code></pre> <ul> <li>\u901a\u8fc7 <code>curl</code> \u547d\u4ee4\u67e5\u770b\u5f53\u524d\u7684\u6a21\u578b\u5217\u8868</li> </ul> <pre><code>curl http://localhost:8000/v1/models\n</code></pre> <ul> <li>\u4f7f\u7528 <code>curl</code> \u547d\u4ee4\u6d4b\u8bd5 <code>OpenAI Completions API</code> </li> </ul> <pre><code>curl http://localhost:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"DeepSeek-R1-Distill-Qwen-7B\",\n        \"prompt\": \"\u6211\u60f3\u95ee\u4f60\uff0c5\u7684\u9636\u4e58\u662f\u591a\u5c11\uff1f&lt;think&gt;\\n\",\n        \"max_tokens\": 1024,\n        \"temperature\": 0\n    }'\n</code></pre> <ul> <li>\u7528 <code>Python</code> \u811a\u672c\u8bf7\u6c42 <code>OpenAI Completions API</code> </li> </ul> <pre><code># vllm_openai_completions.py\nfrom openai import OpenAI\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"empty\", # \u968f\u4fbf\u586b\u5199\uff0c\u53ea\u662f\u4e3a\u4e86\u901a\u8fc7\u63a5\u53e3\u53c2\u6570\u6821\u9a8c\n)\n\ncompletion = client.chat.completions.create(\n  model=\"deepseek-qwen-1.5b\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"\u6211\u60f3\u95ee\u4f60\uff0c5\u7684\u9636\u4e58\u662f\u591a\u5c11\uff1f&lt;think&gt;\\n\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre> <pre><code>python vllm_openai_completions.py\n</code></pre> <ul> <li><code>request</code>\u65b9\u5f0f</li> </ul> <pre><code># chat_vllm_api.py\nimport requests\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\nURL = \"http://localhost:8000/v1/chat/completions\"\n\nprompt = {\n    \"model\": \"deepseek-qwen-1.5b\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a large model that excels at mathematical and physical reasoning.\"},\n        {\"role\": \"user\", \"content\": \"Please introduce Pade approximants for me. Please reason step by step, and put your final answer within \\boxed{}.\"}\n    ],\n    \"temperature\": 0.6,\n    \"top_p\": 0.95\n}\n\nresp = requests.post(URL, headers=headers, json=prompt, stream=False)\nrep = resp.json()\nprint(rep)\n</code></pre> <ul> <li>\u7528 <code>Python</code> \u811a\u672c\u8bf7\u6c42 <code>OpenAI Chat Completions API</code> </li> </ul> <pre><code># vllm_openai_chat_completions.py\nfrom openai import OpenAI\nopenai_api_key = \"empty\" # \u968f\u4fbf\u586b\u5199\uff0c\u53ea\u662f\u4e3a\u4e86\u901a\u8fc7\u63a5\u53e3\u53c2\u6570\u6821\u9a8c\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_outputs = client.chat.completions.create(\n    model=\"deepseek-qwen-1.5b\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\uff1f\"},\n    ]\n)\nprint(chat_outputs)\n</code></pre> <pre><code>python vllm_openai_chat_completions.py\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#_8","title":"\u63a8\u7406\u901f\u5ea6\u6d4b\u8bd5","text":"<p>\u65e2\u7136 <code>vLLM</code> \u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u90e8\u7f72\u670d\u52a1\u7cfb\u7edf\uff0c\u90a3\u4e48\u6211\u4eec\u4e0d\u59a8\u5c31\u6d4b\u8bd5\u4e00\u4e0b\u6a21\u578b\u7684\u56de\u590d\u751f\u6210\u901f\u5ea6\u3002\u770b\u770b\u548c\u539f\u59cb\u7684\u901f\u5ea6\u76f8\u6bd4\u6709\u591a\u5927\u7684\u63d0\u5347\u3002\u8fd9\u91cc\u76f4\u63a5\u4f7f\u7528 <code>vLLM</code> \u81ea\u5e26\u7684 <code>benchmark_throughput.py</code> \u811a\u672c\u8fdb\u884c\u6d4b\u8bd5\u3002\u53ef\u4ee5\u5c06\u5f53\u524d\u6587\u4ef6\u5939 <code>benchmark_throughput.py</code> \u811a\u672c\u653e\u5728 <code>/root/autodl-tmp/</code> \u76ee\u5f55\u4e0b\uff1b\u6216\u8005\u4e5f\u53ef\u4ee5\u81ea\u884c\u4e0b\u8f7d\u6700\u65b0\u7248\u811a\u672c</p> <p>\u4e0b\u9762\u662f\u4e00\u4e9b <code>benchmark_throughput.py</code> \u811a\u672c\u7684\u53c2\u6570\u8bf4\u660e\uff1a</p> <ul> <li><code>--model</code> \u53c2\u6570\u6307\u5b9a\u6a21\u578b\u8def\u5f84\u6216\u540d\u79f0\u3002</li> <li><code>--backend</code> \u63a8\u7406\u540e\u7aef\uff0c\u53ef\u4ee5\u662f <code>vllm</code>\u3001<code>hf</code> \u548c <code>mii</code>\u3002\u5206\u5e03\u5bf9\u5e94 <code>vLLM</code>\u3001<code>HuggingFace</code> \u548c <code>Mii</code> \u63a8\u7406\u540e\u7aef\u3002</li> <li><code>--input-len</code> \u8f93\u5165\u957f\u5ea6</li> <li><code>--output-len</code> \u8f93\u51fa\u957f\u5ea6</li> <li><code>--num-prompts</code> \u751f\u6210\u7684 prompt \u6570\u91cf</li> <li><code>--seed</code> \u968f\u673a\u79cd\u5b50</li> <li><code>--dtype</code> \u6570\u636e\u7c7b\u578b</li> <li><code>--max-model-len</code> \u6a21\u578b\u6700\u5927\u957f\u5ea6</li> <li><code>--hf_max_batch_size</code> <code>transformers</code> \u5e93\u7684\u6700\u5927\u6279\u5904\u7406\u5927\u5c0f\uff08\u4ec5\u4ec5\u5bf9\u4e8e <code>hf</code> \u63a8\u7406\u540e\u7aef\u6709\u6548\u4e14\u4e3a\u5fc5\u586b\u5b57\u6bb5\uff09</li> <li><code>--dataset</code> \u6570\u636e\u96c6\u8def\u5f84\u3002\uff08\u5982\u672a\u8bbe\u7f6e\u4f1a\u81ea\u52a8\u751f\u6210\u6570\u636e\uff09</li> </ul> <p>\u6d4b\u8bd5 <code>vLLM</code> \u63a8\u7406\u901f\u5ea6\u7684\u547d\u4ee4\u548c\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python benchmark_throughput.py \\\n    --model /root/autodl-tmp/qwen/DeepSeek-R1-Distill-Qwen-7B \\\n    --backend vllm \\\n    --input-len 64 \\\n    --output-len 128 \\\n    --num-prompts 25 \\\n    --seed 2025 \\\n  --dtype float16 \\\n  --max-model-len 512\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#vllm-reference","title":"vLLM Reference","text":"<ul> <li>https://github.com/datawhalechina/self-llm/blob/master/models/DeepSeek-R1-Distill-Qwen/</li> <li>https://docs.vllm.ai/en/latest/api/offline_inference/llm.html</li> </ul>"},{"location":"LLMdeploy/LLMdeploy/#vllmsglang-docker","title":"vLLM/SGLang \u5b89\u88c5\u65b9\u5f0f\u4e8c\uff1a\u901a\u8fc7\u62c9\u53d6\u5b98\u65b9\u7684 Docker \u5bb9\u5668\u5b89\u88c5","text":"<p>\u5728GPU\u4e0a\u8fd0\u884c\u7684\u8bdd\u9700\u8981\u5728GPU\u8282\u70b9\u5b89\u88c5 docker \u548c nvidia-container-toolkit\uff0c\u5b89\u88c5\u9700\u8981\u7ba1\u7406\u5458\u6743\u9650\u3002</p> <p>\u7531\u4e8e<code>DockerHub</code>\u662f\u56fd\u5916\u7f51\u7ad9\uff0c\u76f4\u63a5\u8fd0\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u4f1a\u62a5\u9519\uff0c</p> <pre><code>docker pull vllm/vllm-openai\n</code></pre> <p>\u6362\u6210\u56fd\u5185\u955c\u50cf\u6e90\u624d\u80fd\u987a\u5229\u4e0b\u8f7d\uff0c</p> <pre><code>docker pull docker.1ms.run/vllm/vllm-openai\ndocker pull docker.1ms.run/lmsysorg/sglang:latest\ndocker pull docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124\ndocker pull docker.1ms.run/dyrnq/open-webui\ndocker pull docker.1ms.run/savatar101/marker-api\ndocker pull docker.1ms.run/anetaco/marker:v108\ndocker pull registry.cn-beijing.aliyuncs.com/anetaco/marker:v108\ndocker pull registry.cn-beijing.aliyuncs.com/quincyqiang/mineru:0.1-models\ndocker pull registry.cn-beijing.aliyuncs.com/savatar101/marker-api\n</code></pre> <p>\u955c\u50cf\u6e90\u7684\u7f51\u5740\u7ecf\u5e38\u6362\uff0c\u9700\u8981\u81ea\u5df1\u641c\u7d22<code>DockerHub \u56fd\u5185\u955c\u50cf\u6e90</code>\u5bfb\u627e\u53ef\u7528\u7684\u955c\u50cf\u6e90\u7f51\u5740\uff0c\u7528\u6cd5\u7c7b\u4f3c\u3002</p> <p>ln02\u8282\u70b9</p> <pre><code>docker save -o /dssg/work/kfye/docker/images_tar/vllm0.7.2.tar docker.1ms.run/vllm/vllm-openai\ndocker save -o /dssg/work/kfye/docker/images_tar/sglang0.4.2.post4-cu124.tar docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124\n</code></pre> <p>gpu039\u8282\u70b9</p> <pre><code>docker load -i /dssg/work/kfye/docker/images_tar/vllm0.7.2.tar\ndocker load -i /dssg/work/kfye/docker/images_tar/sglang0.4.2.post4-cu124.tar\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#sglang","title":"sglang \u53cc\u8282\u70b9\u90e8\u7f72","text":"<p>\u7a0b\u5e8f\u5458\u963f\u8d5e</p>"},{"location":"LLMdeploy/LLMdeploy/#1-gpu039","title":"\u8282\u70b91 gpu039","text":"<pre><code>docker run --gpus all \\\n    --shm-size 10g \\\n    --network=host \\\n    -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --name sglang2a \\\n    -p 1234:1234 -d \\\n    --ipc=host --restart always \\\n    docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1 --served-model-name deepseek-r1 --tp 16 --dist-init-addr 11.11.11.39:4321 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 1234\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#2-gpu040","title":"\u8282\u70b92 gpu040","text":"<pre><code>docker run --gpus all \\\n    --shm-size 10g \\\n    --network=host \\\n    -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --name sglang2a \\\n    -p 1234:1234 -d \\\n    --ipc=host --restart always \\\n    docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1 --served-model-name deepseek-r1 --tp 16 --dist-init-addr 11.11.11.39:4321 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 1234\n</code></pre> <p>optional</p> <pre><code>--privileged -e NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8 -e NCCL_P2P_LEVEL=NVL -e NCCL_IB_GID_INDEX=0  -e NCCL_IB_CUDA_SUPPORT=1 -e library/NCCL_IB_DISABLE=0 -e NCCL_SOCKET_IFNAME=ibs11,ibs12,ibs13,ibs14,ibs15,ibs16,ibs17,ibs18 -e NCCL_DEBUG=INFO -e NCCL_NET_GDR_LEVEL=2 \\\n--privileged -e NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8 -e NCCL_P2P_LEVEL=NVL -e NCCL_IB_GID_INDEX=0  -e NCCL_IB_CUDA_SUPPORT=1 -e NCCL_IB_DISABLE=0 -e NCCL_SOCKET_IFNAME=ibs11,ibs12,ibs13,ibs14,ibs15,ibs16,ibs17,ibs18 -e NCCL_DEBUG=INFO -e NCCL_NET_GDR_LEVEL=2 \\\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#sglang_1","title":"sglang \u5b98\u65b9","text":"<pre><code>docker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --name sglang1 \\\n    -it \\\n    --rm \\\n    --ipc=host \\\n    docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1 --tp 16 --dist-init-addr 10.10.10.39:20000 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 40000 --output-file \"deepseekr1.jsonl\"\n\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#_9","title":"\u5355\u4e00\u8282\u70b9","text":"<pre><code># gpu039\ndocker run --gpus all --shm-size 10g --name sglang1b \\\n    -p 22:22 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.39 --port 22\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint sglang1b (265c3c57cc5f76d4304a66097f4e000a415bb9f7fcf440123246a7766ca1f0dd): failed to bind port 0.0.0.0:22/tcp: Error starting userland proxy: listen tcp4 0.0.0.0:22: bind: address already in use.\n\ndocker run --gpus all --shm-size 10g --name sglang1a \\\n    -p 1234:1234 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.39 --port 1234\n[2025-02-17 11:07:35] ERROR:    [Errno 99] error while attempting to bind on address ('11.11.11.39', 1234): cannot assign requested address\n\ndocker run --gpus all --shm-size 10g --name sglang1a \\\n    -p 22:4321 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.39 --port 4321\n\ndocker run --gpus all --shm-size 10g --name sglang1d \\\n    -p 1234:1234 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --network host --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.39 --port 1234\n\u5df2\u7ecf\u5728gpu039\u8282\u70b9\u6210\u529f\u90e8\u7f72\u3002\n\u90e8\u7f72qwen-32b\ndocker cp /dssg/work/kfye/qwen/qwen32b sglang_gpu4:/sgl-workspace/\nCUDA_VISIBLE_DEVICES=6,7 python3 -m sglang.launch_server --model-path /sgl-workspace/qwen32b --served-model-name qwen-32b --tp 2 --trust-remote-code --host 0.0.0.0 --port 1234\n\n\ndocker run --gpus all --shm-size 32g --name sglang_gpu4 -itd \\\n    -v /dssg/work/kfye/deepseek/deepseek-ai/DeepSeek-R1-Distill-Llama-70B:/sgl-workspace/DeepSeek-R1-Distill-Llama-70B \\\n    -v /dssg/work/kfye/deepseek/deepseek-ai/pychat:/sgl-workspace/pychat \\\n    --network host --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124\n\n# gpu040\ndocker run --gpus all --shm-size 10g --name sglang1b \\\n    -p 1234:1234 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.40 --port 1234\n[2025-02-17 10:22:33] ERROR:    [Errno 99] error while attempting to bind on address ('11.11.11.40', 1234): cannot assign requested address\n\ndocker run --gpus all --shm-size 10g --name sglang1c \\\n    -p 1234:1234 -v /dssg/work/kfye/deepseek/deepseek-ai:/sgl-workspace/deepseek-ai \\\n    --network host --ipc=host docker.1ms.run/lmsysorg/sglang:v0.4.2.post4-cu124 \\\n    python3 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeek-R1-Distill-Llama-70B --tp 8 --trust-remote-code --host 11.11.11.40 --port 1234\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#api","title":"API\u670d\u52a1\u90e8\u7f72\u65b9\u5f0f","text":"<pre><code>\u5728gpu039\u8282\u70b9:\ndocker ps -a\ndocker start sglang1a\nnvidia-smi # \u76f4\u5230gpu\u663e\u5b58\u5229\u7528\u7387\u8fbe\u523090%\u5de6\u53f3\u5c31\u662fapi\u670d\u52a1\u90e8\u7f72\u5b8c\u6210\u4e86\ndocker logs sglang1a # \u62a5\u9519\u4e0d\u7528\u7ba1\uff0c\u662f\u4e0a\u6b21 docker stop sglang1a \u5e26\u6765\u7684\uff0c\u7b49\u4e00\u4f1a\u5c31\u4f1a\u51fa\u73b0\u52a0\u8f7d\u6a21\u578b\u7684\u4fe1\u606f\uff0c\u9700\u8981\u5341\u51e0\u5206\u949f\ndocker cp /dssg/work/kfye/deepseek/openai_chat_completion_streaming.py sglang2:/sgl-workspace/\ndocker exec -it sglang1a bash\npython3 openai_chat_completion_streaming.py\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#open-webui","title":"open-webui \u90e8\u7f72","text":"<p>\u7531\u4e8e<code>DockerHub</code>\u662f\u56fd\u5916\u7f51\u7ad9\uff0c\u76f4\u63a5\u8fd0\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u4f1a\u62a5\u9519\uff0c</p> <pre><code>docker pull vllm/vllm-openai\n</code></pre> <p>\u6362\u6210\u56fd\u5185\u955c\u50cf\u6e90\u624d\u80fd\u987a\u5229\u4e0b\u8f7d\uff0c</p> <pre><code>docker pull docker.1ms.run/dyrnq/open-webui\n</code></pre> <p>\u955c\u50cf\u6e90\u7684\u7f51\u5740\u7ecf\u5e38\u6362\uff0c\u9700\u8981\u81ea\u5df1\u641c\u7d22<code>DockerHub \u56fd\u5185\u955c\u50cf\u6e90</code>\u5bfb\u627e\u53ef\u7528\u7684\u955c\u50cf\u6e90\u7f51\u5740\uff0c\u7528\u6cd5\u7c7b\u4f3c\u3002</p> <pre><code>docker save -o /dssg/work/kfye/docker/images_tar/openwebui.tar docker.1ms.run/dyrnq/open-webui\ndocker load -i /dssg/work/kfye/docker/images_tar/openwebui.tar\ndocker run -d -p 1234:8080 -v open-webui:/app/backend/data --name open-webui docker.1ms.run/dyrnq/open-webui\n</code></pre>"},{"location":"LLMdeploy/LLMdeploy/#curl","title":"curl","text":"<pre><code>curl http://122.204.190.7:5678/v1/chat/completions \\\ncurl http://122.204.190.7:8555/v1/chat/completions \\\ncurl http://122.204.190.7:1234/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-32b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Why the sky is blue?\\n\"}\n    ]\n}'\n\n\n\"model\": \"minicpm-v:8b-2.6-fp16\",\n\"model\": \"DeepSeek-R1-Distill-Llama-70B\",\n\"model\": \"deepseek-r1-70b:latest\",\n\n\ncurl http://11.11.11.40:11434/v1/chat/completions \\\ncurl http://11.11.11.39:1234/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"qwen-32b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Why the sky is blue?&lt;think&gt;\\n\"}\n    ]\n}'\n\ncurl http://127.0.0.1:11434/api/chat -d '{\ncurl http://localhost:11434/api/chat -d '{\ncurl http://11.11.11.40:11434/api/chat -d '{\ncurl http://122.204.190.7:11434/api/chat -d '{\n  \"model\": \"qwen2.5:7b-instruct-120k\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ],\n  \"stream\": false\n}'\n\n\ncurl --request POST \\\n     --url http://122.204.190.7:8380/api/v1/chats/ad54d9d2f02e11ef90fa0242ac120006/sessions \\\n     --header 'Content-Type: application/json' \\\n     --header 'Authorization: Bearer ragflow-JlMjRkOWYyZjAzMjExZWZiOTBiMDI0Mm' \\\n     --data '{\n    \"name\":\"new_chat_1\"\n}'\n\ncurl --request POST \\\n     --url http://122.204.190.7:8380/api/v1/chats/ad54d9d2f02e11ef90fa0242ac120006/sessions \\\n     --header 'Content-Type: application/json' \\\n     --header 'Authorization: Bearer ragflow-JlMjRkOWYyZjAzMjExZWZiOTBiMDI0Mm' \\\n     --data-binary '\n     {\n          \"name\":\"new_chat_1\"\n          \"question\": \"Who are you\",\n          \"stream\": true,\n     }'\n\n\"session_id\":\"9fa7691cb85c11ef9c5f0242ac120005\"\n\"dataset_ids\": [\"0b2cbc8c877f11ef89070242ac120005\"],\n\nad54d9d2f02e11ef90fa0242ac120006\n</code></pre>"},{"location":"RAG/marker-pdf/","title":"marker-pdf\u7528\u6cd5","text":"<ul> <li>marker-pdf \u5b89\u88c5\u4e0e\u4f7f\u7528\u6307\u5357</li> <li>\u73af\u5883\u914d\u7f6e\u4e0e\u5b89\u88c5</li> <li>\u591a GPU \u73af\u5883\u4e0b\u7684 PDF \u5904\u7406<ul> <li>\u914d\u7f6e\u6587\u4ef6\u4fee\u6539</li> <li>marker_chunk_convert \u547d\u4ee4\u8be6\u89e3</li> </ul> </li> <li>\u8c03\u7528 ollama \u90e8\u7f72\u7684 LLM \u4f18\u5316OCR\u7ed3\u679c<ul> <li>Ollama \u670d\u52a1\u90e8\u7f72</li> <li>\u6a21\u578b\u541e\u5410\u6027\u80fd\u8bc4\u6d4b</li> <li>\u6c38\u4e45\u4fee\u6539ollama\u6a21\u578b\u53c2\u6570\uff1a\u521b\u5efa\u65b0\u6a21\u578b</li> <li>ollama\u7528\u672c\u5730\u6a21\u578b\u6743\u91cd\u521b\u5efa\u65b0\u6a21\u578b</li> <li>PDF \u5904\u7406\u793a\u4f8b</li> </ul> </li> </ul>"},{"location":"RAG/marker-pdf/#marker-pdf","title":"marker-pdf \u5b89\u88c5\u4e0e\u4f7f\u7528\u6307\u5357","text":""},{"location":"RAG/marker-pdf/#_1","title":"\u73af\u5883\u914d\u7f6e\u4e0e\u5b89\u88c5","text":"<p>\u9996\u5148\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 conda \u73af\u5883\u5e76\u5b89\u88c5\u5fc5\u8981\u7684\u4f9d\u8d56\uff1a</p> <pre><code>conda create -n marker python=3.12 -y\nconda activate marker\nmodule load cuda-12.4\npip install torch\npip install marker-pdf\n</code></pre> <p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u9a8c\u8bc1\u5b89\u88c5\u8def\u5f84\uff1a</p> <pre><code>(marker) [kfye@gpu040 kfye]$ which marker\n/dssg/work/kfye/anaconda3/envs/marker/bin/marker\n(marker) [kfye@gpu040 kfye]$ which marker_single\n/dssg/work/kfye/anaconda3/envs/marker/bin/marker_single\n(marker) [kfye@gpu040 kfye]$ which marker_chunk_convert\n/dssg/work/kfye/anaconda3/envs/marker/bin/marker_chunk_convert\n(marker) [kfye@gpu040 kfye]$ which conda\n/dssg/work/kfye/anaconda3/condabin/conda\n</code></pre>"},{"location":"RAG/marker-pdf/#gpu-pdf","title":"\u591a GPU \u73af\u5883\u4e0b\u7684 PDF \u5904\u7406","text":""},{"location":"RAG/marker-pdf/#_2","title":"\u914d\u7f6e\u6587\u4ef6\u4fee\u6539","text":"<p>\u9996\u5148\u9700\u8981\u4fee\u6539\u8f6c\u6362\u811a\u672c\u7684\u914d\u7f6e\uff1a</p> <pre><code>vim /dssg/work/kfye/anaconda3/envs/marker/lib/python3.12/site-packages/marker/scripts/chunk_convert.sh\n</code></pre> <p>\u5173\u952e\u914d\u7f6e\u5185\u5bb9\uff1a</p> <pre><code>cmd=\"CUDA_VISIBLE_DEVICES=$DEVICE_NUM marker $INPUT_FOLDER --output_dir $OUTPUT_FOLDER --num_chunks $NUM_DEVICES --chunk_idx $DEVICE_NUM --workers $NUM_WORKERS --output_format markdown --paginate_output --force_ocr --languages \\\"en,zh\\\"\"\n# cmd=\"CUDA_VISIBLE_DEVICES=$DEVICE_NUM marker $INPUT_FOLDER --output_dir $OUTPUT_FOLDER --num_chunks $NUM_DEVICES --chunk_idx $DEVICE_NUM --workers $NUM_WORKERS\"\n</code></pre>"},{"location":"RAG/marker-pdf/#marker_chunk_convert","title":"marker_chunk_convert \u547d\u4ee4\u8be6\u89e3","text":"<pre><code># \u4f7f\u7528 4 \u5f20GPU\u5361\uff0c\u6bcf\u5f20\u5361 3 \u4e2a\u5de5\u4f5c\u8fdb\u7a0b\ntime NUM_DEVICES=4 NUM_WORKERS=3 marker_chunk_convert /dssg/work/kfye/marker/pdfs /dssg/work/kfye/marker/mds &gt; /dssg/work/kfye/marker/log_marker.txt 2&gt;&amp;1 &amp;\ntail -f /dssg/work/kfye/marker/log_marker.txt\n</code></pre> <p>\u53c2\u6570\u8be6\u89e3\uff1a - <code>NUM_DEVICES=2</code>\u4ee3\u8868\u8c03\u7528\u524d\u4e24\u5f20GPU\u5361\u3002\u9ed8\u8ba4<code>CUDA_VISIBLE_DEVICES</code>\u4ece0\u5f00\u59cb\u5206\u914d\uff0c\u547d\u4ee4\u524d\u9762\u52a0<code>CUDA_VISIBLE_DEVICES=3</code>\u4e5f\u6ca1\u7528\uff0c\u53ea\u80fd\u81ea\u5df1\u6539<code>marker/scripts/chunk_convert.sh</code>\u3002 - \u4e00\u4e2apdf\u53ea\u80fd\u5728\u4e00\u5f20GPU\u5361\u4e0a\u8dd1\uff0c\u76ee\u5f55\u4e0b\u6709\u591a\u4e2aPDF\u7684\u8bdd\uff0cmarker\u4f1a\u81ea\u52a8\u5206\u5230\u56db\u4e2aGPU\uff08\u6bd4\u598217\u4e2aPDF\u81ea\u52a8\u5206\u62105+5+5+2\u57284\u5f20\u5361\u8dd1\uff0c\u81ea\u52a8\u5206\u62109+8\u57282\u5f20\u5361\u8dd1\uff09\u3002 - <code>marker_chunk_convert</code>\u524d\u9762\u6dfb\u52a0<code>NUM_WORKERS=3</code>\u6307\u7684\u662f\u6bcf\u5f20GPU\u53613\u4e2a\u8fdb\u7a0b\u3002 - <code>NUM_WORKERS=1</code>\u4ee3\u8868\u5355\u4e00\u8fdb\u7a0b\uff0cgpu\u5229\u7528\u7387\u5927\u90e8\u4efd\u65f6\u95f4\u90fd\u572815%-20%\uff0c\u5076\u5c14\u80fd\u523050%\u3002\u5355\u8fdb\u7a0b\u663e\u5b58\u5360\u7528 2523MB-3348MB - <code>NUM_WORKERS</code>\u8bbe\u7f6e\u6210\u5927\u4e8e\u7b49\u4e8e4\uff0c\u53ef\u4ee5\u628aGPU\u5229\u7528\u7387\u62c9\u6ee1\uff0c\u4f46\u662f\u4f1a\u62a5\u9519<code>libgomp: Thread creation failed: Resource temporarily unavailable</code></p> <p>\u60f3\u4e2d\u65ad<code>marker_chunk_convert</code>\u8fdb\u7a0b\u7684\u8bdd\uff0c<code>Ctrl+C</code>\u4e0d\u7ba1\u7528\uff0c\u53ea\u80fd<code>pgrep -f marker | xargs kill -9</code>\uff0c\u5efa\u8bae\u5148\u7528<code>pgrep -fl marker</code>\u770b\u4e00\u4e0b\u8fdb\u7a0b\u7684\u5b8c\u6574\u540d\u79f0\uff0c\u522b\u8bef\u6740\u8fdb\u7a0b\u4e86\u3002</p>"},{"location":"RAG/marker-pdf/#ollama-llm-ocr","title":"\u8c03\u7528 ollama \u90e8\u7f72\u7684 LLM \u4f18\u5316OCR\u7ed3\u679c","text":""},{"location":"RAG/marker-pdf/#ollama","title":"Ollama \u670d\u52a1\u90e8\u7f72","text":"<pre><code>CUDA_VISIBLE_DEVICES=6,7 OLLAMA_NUM_PARALLEL=12 \\\nOLLAMA_KEEP_ALIVE=24h OLLAMA_HOST=0.0.0.0:11434 \\\nollama serve &gt; /dssg/work/kfye/marker/log_ollama_serve.txt 2&gt;&amp;1 &amp;\n</code></pre>"},{"location":"RAG/marker-pdf/#_3","title":"\u6a21\u578b\u541e\u5410\u6027\u80fd\u8bc4\u6d4b","text":"<p>PDF\u8f6cmd\u7684\u901f\u5ea6\u53d7\u9650\u4e8eOllama\u90e8\u7f72\u7684LLM\u7684\u541e\u5410\u901f\u5ea6\u3002\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u6d4b\u8bd5\u7ed3\u679c\uff1a</p> <pre><code># \u5404\u6a21\u578b\u7684\u8bc4\u6d4b\u7ed3\u679c\nollama run deepseek-r1:32b-qwen-distill-fp16 --verbose # eval rate: 21.51 tokens/s\nollama run qwen2.5:72b-instruct --verbose # eval rate: 22.00 tokens/s\nollama run qwen2.5:1.5b # eval rate: 177.93 tokens/s\nollama run qwen2.5:7b-instruct # eval rate: 109.82 tokens/s\nollama create deepseek-r1-q4km # fail\n# msg=\"model missing blk.0 layer size\"\n# panic: interface conversion: interface {} is nil, not *llm.array\n</code></pre>"},{"location":"RAG/marker-pdf/#ollama_1","title":"\u6c38\u4e45\u4fee\u6539ollama\u6a21\u578b\u53c2\u6570\uff1a\u521b\u5efa\u65b0\u6a21\u578b","text":"<pre><code>CUDA_VISIBLE_DEVICES=1,2 OLLAMA_NUM_PARALLEL=2 \\\nOLLAMA_KEEP_ALIVE=24h OLLAMA_HOST=0.0.0.0:11434 \\\nollama serve &gt; /dssg/work/kfye/marker/log_ollama_serve.txt 2&gt;&amp;1 &amp;\n\nollama show --modelfile qwen2.5:7b-instruct &gt; Modelfile\nvim Modelfile\n......\nFROM qwen2.5:7b-instruct\nPARAMETER temperature 0.6\nPARAMETER num_ctx 120000\n......\nollama create qwen2.5:7b-instruct-120k -f Modelfile\n</code></pre>"},{"location":"RAG/marker-pdf/#ollama_2","title":"ollama\u7528\u672c\u5730\u6a21\u578b\u6743\u91cd\u521b\u5efa\u65b0\u6a21\u578b","text":"<pre><code># deepseek-r1-70b, f16\u7cbe\u5ea6\ncd /path/to/safetensor_or_gguf_files\nvim Modelfile\n---\nFROM .\n\nPARAMETER num_predict 500\nPARAMETER temperature 0.6\nPARAMETER num_batch 128\nPARAMETER num_ctx 127000\n---\nollama create deepseek-r1-70b\nollama create deepseek-r1-q4km # fail\n# msg=\"model missing blk.0 layer size\"\n# panic: interface conversion: interface {} is nil, not *llm.array\n\n</code></pre>"},{"location":"RAG/marker-pdf/#pdf","title":"PDF \u5904\u7406\u793a\u4f8b","text":"<p>\u4f7f\u7528 LLM \u5904\u7406\u5355\u4e2a PDF \u6587\u4ef6\uff1a</p> <pre><code>CUDA_VISIBLE_DEVICES=5 marker_single /dssg/work/kfye/marker/pdf/LQCD_liuchuan.pdf \\\n--output_dir /dssg/work/kfye/marker/md_llm/qwen7b \\\n--output_format markdown --paginate_output --force_ocr --languages \"en,zh\" \\\n--use_llm --ollama_base_url http://0.0.0.0:11434 \\\n--ollama_model qwen2.5:7b-instruct-120k \\\n--llm_service=marker.services.ollama.OllamaService &gt; /dssg/work/kfye/marker/log_qwen7b_liuchuan.txt 2&gt;&amp;1 &amp;\n</code></pre> <p>\u6027\u80fd\u8bf4\u660e\uff1a - Ollama \u670d\u52a1\u4f7f\u7528\u4e24\u5757 GPU\uff0c\u8bbe\u7f6e OLLAMA_NUM_PARALLEL=4\uff0cGPU \u5229\u7528\u7387\u7ef4\u6301\u5728 50% \u4ee5\u4e0a\uff0c\u53ef\u4ee5\u901a\u8fc7\u8c03\u5927 OLLAMA_NUM_PARALLEL \u63d0\u9ad8\u5229\u7528\u7387\u3002</p> <p>\u540e\u53f0\u8fdb\u7a0b\u4fe1\u606f\uff1a</p> <pre><code>/dssg/work/kfye/deepseek/ollama/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /dssg/work/kfye/.ollama/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --ctx-size 4096 --batch-size 512 --n-gpu-layers 29 --threads 128 --parallel 2 --port 33959\n</code></pre>"},{"location":"RAG/ragflow_opt/","title":"ragflow \u8c03\u4f18\u7b56\u7565","text":""},{"location":"RAG/ragflow_opt/#pdf-to-md","title":"pdf to md","text":"<p>olmocr\u6bd4marker\u66f4\u5f3a\u5927\u3002 https://olmocr.allenai.org/</p>"},{"location":"RAG/ragflow_opt/#embedding-model","title":"embedding model","text":"<ul> <li>PhysBERT</li> </ul> <p>PhysBERT is a specialized text embedding model for physics, designed to improve information retrieval, citation classification, and clustering of physics literature. Trained on 1.2 million physics papers, it outperforms general-purpose models in physics-specific tasks.</p>"},{"location":"RAG/ragflow_opt/#_1","title":"\u5206\u5757\u7b56\u7565","text":"<p>\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u6863\u9009\u53d6\u4e0d\u540c\u7684chunk size.</p>"},{"location":"RAG/ragflow_opt/#graphrag","title":"GraphRAG \u7684\u56f0\u96be","text":"<ul> <li>\u8017\u65f6\u957f\uff0c\u5218\u5ddd\u683c\u70b9\u6559\u6750\u7b49\u4e86\u51e0\u4e2a\u5c0f\u65f6\u4e5f\u6ca1\u89e3\u6790\u5b8c\u3002</li> <li>\u9700\u8981\u81ea\u5df1\u624b\u52a8\u6dfb\u52a0\u5b9e\u4f53\u7c7b\u522b\uff08entity type\uff09\u3002</li> </ul>"},{"location":"RAG/ragflow_opt/#_2","title":"\u63d0\u793a\u8bcd\u5de5\u7a0b","text":"<p>CO-STAR</p> <p>Context \u8bf7\u4f60\u6839\u636e\u95ee\u9898\u641c\u7d22\u8d44\u6599\u4f5c\u4e3a\u80cc\u666f\u77e5\u8bc6\u3002</p> <p>Objective \u8bf7\u5b8c\u6210\u4ee5\u4e0b\u590d\u5408\u4efb\u52a1\uff1a 1. \u4efb\u52a11 2. \u4efb\u52a12</p> <p>Style \u6a21\u4effRichard Feynmann\u7684\u5199\u4f5c\u98ce\u683c</p> <p>Tone \u4fdd\u6301\u4e25\u8c28\u5ba2\u89c2\u7684\u5b66\u672f\u8868\u8fbe\uff0c\u4f46\u9700\u5728\u8ba8\u8bba\u73af\u8282\u4f53\u73b0\u521b\u65b0\u6027\u6279\u5224\u601d\u7ef4\u3002\u5bf9\u77db\u76fe\u6570\u636e\u91c7\u7528\"\u53ef\u80fd\u6697\u793a...\"\"\u4e0d\u6392\u9664...\"\u7b49\u8bd5\u63a2\u6027\u8868\u8ff0\uff0c\u7ed3\u8bba\u90e8\u5206\u4f7f\u7528\"\u5f3a\u70c8\u652f\u6301/\u5c1a\u65e0\u6cd5\u8bc1\u5b9e\"\u7b49\u5206\u7ea7\u8bba\u65ad\u3002</p> <p>Audience \u76ee\u6807\u53d7\u4f17\u4e3a\u7269\u7406\u5b66\u4e13\u5bb6\uff0c\u8bf7\u4f60\u7528\u4e13\u4e1a\u7684\u8bed\u8a00\u8bb2\u6e05\u695a\u7ec6\u8282\u3002</p> <p>Response \u8f93\u51fa\u7ed3\u6784\u8981\u6c42\uff1amarkdown\u683c\u5f0f\u3002</p>"},{"location":"RAG/ragflow_opt/#deepsearcher","title":"DeepSearcher","text":""}]}